<!DOCTYPE html>
<html lang="en">
<head>
  <link href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700&effect=destruction%7Cshadow-multiple" rel="stylesheet" type="text/css">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>varnishd: Http accelerator daemon</title>
  <link href="/css/bootstrap.min.css" rel="stylesheet">
  <link href="/css/manpage.css" rel="stylesheet">
  <link rel="apple-touch-icon" sizes="57x57" href="/icons/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/icons/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/icons/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/icons/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/icons/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/icons/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/icons/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/icons/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192" href="/icons/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/icons/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png">
  <link rel="manifest" href="/icons/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/icons/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">
  <meta name="description" content="Http accelerator daemon">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@CartaTech">
  <meta name="twitter:creator" content="@CartaTech">
  <meta name="twitter:title" content="varnishd (1) manual">
  <meta name="twitter:description" content="Http accelerator daemon">
  <meta name="twitter:image" content="https://www.carta.tech/images/varnish-varnishd-1.png">
  <meta property="og:url" content="https://www.carta.tech/man-pages/man1/varnishd.1.html" />
  <meta property="og:type" content="website" />
  <meta property="og:title" content="varnishd (1) manual" />
  <meta property="og:description" content="Http accelerator daemon" />
  <meta property="fb:app_id" content="1241677679199500" />
  <meta property="og:image" content="https://www.carta.tech/images/varnish-varnishd-1.png" />
  <meta property="og:image:width" content="600" />
  <meta property="og:image:height" content="315" />
</head>
<body>
  <div class="container final">
          <div class="page-header">
        <h1 class="font-effect-destruction">varnishd<small> (1)</small></h1>
        <p class="lead">Http accelerator daemon</p>
      </div>

    <ol class="breadcrumb" itemscope itemtype="http://schema.org/BreadcrumbList">
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/">
      <span itemprop="name">Carta.tech</span>
    </a>
    <meta itemprop="position" content="1" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/man-pages/">
      <span itemprop="name">Man Pages</span>
    </a>
    <meta itemprop="position" content="2" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/man-pages/man1/">
      <span itemprop="name">Executable programs or shell commands</span>
    </a>
    <meta itemprop="position" content="3" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/man-pages/man1/varnishd.1.html">
      <span itemprop="name">varnishd: Http accelerator daemon</span>
    </a>
    <meta itemprop="position" content="4" />
  </li>
</ol>
<ol class="breadcrumb" itemscope itemtype="http://schema.org/BreadcrumbList">
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/">
      <span itemprop="name">Carta.tech</span>
    </a>
    <meta itemprop="position" content="1" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/packages/">
      <span itemprop="name">Packages</span>
    </a>
    <meta itemprop="position" content="2" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/packages/varnish/">
      <span itemprop="name">varnish</span>
    </a>
    <meta itemprop="position" content="3" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/man-pages/man1/varnishd.1.html">
      <span itemprop="name">varnishd: Http accelerator daemon</span>
    </a>
    <meta itemprop="position" content="4" />
  </li>
</ol>
    
      <section>
        <h2 class="font-effect-shadow-multiple">SYNOPSIS</h2>
        <div class="sectioncontent">

<dl class='dl-vertical'>
  <dt>
    <p><strong>varnishd [-a address[:port]] [-b host[:port]] [-C] [-d] [-f config]</strong></p>
  </dt>
  <dd>
    <p>[-F] [-g group] [-h type[,options]] [-i identity] [-l shl[,free[,fill]]] [-M address:port] [-n name] [-P file] [-p param=value] [-r param[,param...] [-s [name=]kind[,options]] [-S secret-file] [-T address[:port]] [-t ttl] [-u user] [-V]</p>
  </dd>

</dl>

        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">DESCRIPTION</h2>
        <div class="sectioncontent">
<p>The varnishd daemon accepts HTTP requests from clients, passes them on to a backend server and caches the returned documents to better satisfy future requests for the same document.</p>
        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">OPTIONS</h2>
        <div class="sectioncontent">

<dl class='dl-vertical'>
  <dt>
    <p><strong>-a address[:port][,address[:port][...]</strong></p>
  </dt>
  <dd>
    <p>Listen for client requests on the specified address and port.  The address can be a host name (“localhost”), an IPv4 dotted-quad (“127.0.0.1”), or an IPv6 address enclosed in square brackets (“[::1]”).  If address is not specified, varnishd will listen on all available IPv4 and IPv6 interfaces.  If port is not specified, the default HTTP port as listed in /etc/services is used.  Multiple listening addresses and ports can be specified as a whitespace or comma -separated list.</p>
  </dd>
  <dt>
    <p><strong>-b host[:port]</strong></p>
  </dt>
  <dd>
    <p>Use the specified host as backend server.  If port is not specified, the default is 8080.</p>
  </dd>
  <dt>
    <p><strong>-C</strong></p>
  </dt>
  <dd>
    <p>Print VCL code compiled to C language and exit. Specify the VCL file to compile with the -f option.</p>
  </dd>
  <dt>
    <p><strong>-d</strong></p>
  </dt>
  <dd>
    <p>Enables debugging mode: The parent process runs in the foreground with a CLI connection on stdin/stdout, and the child process must be started explicitly with a CLI command. Terminating the parent process will also terminate the child.</p>
  </dd>
  <dt>
    <p><strong>-f</strong><em></em><strong>config</strong></p>
  </dt>
  <dd>
    <p>Use the specified VCL configuration file instead of the builtin default.  See <strong>vcl</strong>(7) for details on VCL syntax. When no configuration is supplied varnishd will not start the cache process.</p>
  </dd>
  <dt>
    <p><strong>-F</strong></p>
  </dt>
  <dd>
    <p>Run in the foreground.</p>
  </dd>
  <dt>
    <p><strong>-g</strong><em></em><strong>group</strong></p>
  </dt>
  <dd>
    <p>Specifies the name of an unprivileged group to which the child process should switch before it starts accepting connections.  This is a shortcut for specifying the group run-time parameter.</p>
  </dd>
  <dt>
    <p><strong>-h type[,options]</strong></p>
  </dt>
  <dd>
    <p>Specifies the hash algorithm.  See Hash Algorithms for a list of supported algorithms.</p>
  </dd>
  <dt>
    <p><strong>-i</strong><em></em><strong>identity</strong></p>
  </dt>
  <dd>
    <p>Specify the identity of the Varnish server.  This can be accessed using server.identity from VCL</p>
  </dd>
  <dt>
    <p><strong>-l shl[,free[,fill]]</strong></p>
  </dt>
  <dd>
    <p>Specifies size of shmlog file. shl is the store for the shared memory log records [80M], free is the store for other allocations [1M] and fill determines how the log is [+]. Scaling suffixes like &apos;k&apos;, &apos;M&apos; can be used up to (E)xabytes.  Default is 80 Megabytes.</p>
  </dd>
  <dt>
    <p><strong>-M address:port</strong></p>
  </dt>
  <dd>
    <p>Connect to this port and offer the command line interface. Think of it as a reverse shell. When running with -M and there is no backend defined the child process (the cache) will not start initially.</p>
  </dd>
  <dt>
    <p><strong>-n</strong><em></em><strong>name</strong></p>
  </dt>
  <dd>
    <p>Specify the name for this instance.  Amonst other things, this name is used to construct the name of the directory in which varnishd keeps temporary files and persistent state. If the specified name begins with a forward slash, it is interpreted as the absolute path to the directory which should be used for this purpose.</p>
  </dd>
  <dt>
    <p><strong>-P</strong><em></em><strong>file</strong></p>
  </dt>
  <dd>
    <p>Write the process&apos;s PID to the specified file.</p>
  </dd>
  <dt>
    <p><strong>-p param=value</strong></p>
  </dt>
  <dd>
    <p>Set the parameter specified by param to the specified value.  See Run-Time Parameters for a list of parameters. This option can be used multiple times to specify multiple parameters.</p>
  </dd>
  <dt>
    <p><strong>-r param[,param...]</strong></p>
  </dt>
  <dd>
    <p>Make the listed parameters read only. This gives the system administrator a way to limit what the Varnish CLI can do. Consider making parameters such as <em>user</em>, <em>group</em>, <em>cc_command</em>, <em>vcc_allow_inline_c</em> read only as these can potentially be used to escalate privileges from the CLI. Protecting <em>listen_address</em> may also be a good idea.</p>
  </dd>
  <dt>
    <p><strong>-s [name=]type[,options]</strong></p>
  </dt>
  <dd>
    
  </dd>
  <dt>
    <p><strong>Use the specified storage backend. The storage backends can be one of the following:</strong></p>
  </dt>
  <dd>
    <ul>
<li><p>malloc[,size]</p></li><li><p>file[,path[,size[,granularity]]]</p></li><li><p>persistent,path,size</p><p>See Storage Types in the Users Guide for more information on the various storage backends.  This option can be used multiple times to specify multiple storage files. Names are referenced in logs, vcl, statistics, etc.</p></li>
</ul>
  </dd>
  <dt>
    <p><strong>-S</strong><em></em><strong>file</strong></p>
  </dt>
  <dd>
    <p>Path to a file containing a secret used for authorizing access to the management port.</p>
  </dd>
  <dt>
    <p><strong>-T address[:port]</strong></p>
  </dt>
  <dd>
    <p>Offer a management interface on the specified address and port.  See Management Interface for a list of management commands.</p>
  </dd>
  <dt>
    <p><strong>-t</strong><em></em><strong>ttl</strong></p>
  </dt>
  <dd>
    <p>Specifies a hard minimum time to live for cached documents. This is a shortcut for specifying the default_ttl run-time parameter.</p>
  </dd>
  <dt>
    <p><strong>-u</strong><em></em><strong>user</strong></p>
  </dt>
  <dd>
    <p>Specifies the name of an unprivileged user to which the child process should switch before it starts accepting connections. This is a shortcut for specifying the user runtime parameter.</p><p>If specifying both a user and a group, the user should be specified first.</p>
  </dd>
  <dt>
    <p><strong>-V</strong></p>
  </dt>
  <dd>
    <p>Display the version number and exit.</p>
  </dd>

</dl>
<h3>Hash Algorithms</h3>
<p>The following hash algorithms are available:</p>
<dl class='dl-vertical'>
  <dt>
    <p><strong>critbit</strong></p>
  </dt>
  <dd>
    
  </dd>
  <dt>
    <p><strong>A self-scaling tree structure. The default hash algorithm in</strong></p>
  </dt>
  <dd>
    <p>Varnish Cache 2.1 and onwards. In comparison to a more traditional B tree the critbit tree is almost completely lockless. Do not change this unless you are certain what you&apos;re doing.</p>
  </dd>
  <dt>
    <p><strong>simple_list</strong></p>
  </dt>
  <dd>
    <p>A simple doubly-linked list.  Not recommended for production use.</p>
  </dd>
  <dt>
    <p><strong>classic[,buckets]</strong></p>
  </dt>
  <dd>
    <p>A standard hash table. The hash key is the CRC32 of the object&apos;s URL modulo the size of the hash table.  Each table entry points to a list of elements which share the same hash key. The buckets parameter specifies the number of entries in the hash table.  The default is 16383.</p>
  </dd>

</dl>

<h3>Storage Types</h3>
<p>The following storage types are available:</p>
<h3>malloc</h3>
<p>syntax: malloc[,size]</p><p>malloc is a memory based backend.</p>
<h3>file</h3>
<p>syntax: file[,path[,size[,granularity]]]</p><p>The file backend stores data in a file on disk. The file will be accessed using mmap.</p>
<h3>persistent (experimental)</h3>
<p>syntax: persistent,path,size</p><p>Persistent storage. Varnish will store objects in a file in a manner that will secure the survival of <em>most</em> of the objects in the event of a planned or unplanned shutdown of Varnish. The persistent storage backend has multiple issues with it and will likely be removed from a future version of Varnish.</p>
<h3>Management Interface</h3>
<p>If the -T option was specified, varnishd will offer a command-line management interface on the specified address and port.  The recommended way of connecting to the command-line management interface is through <strong>varnishadm</strong>(1).</p><p>The commands available are documented in <strong>varnish</strong>(7).</p>
<h3>Run-Time Parameters</h3>
<p>Runtime parameters are marked with shorthand flags to avoid repeating the same text over and over in the table below.  The meaning of the flags are:</p>
<dl class='dl-vertical'>
  <dt>
    <p><strong>experimental</strong></p>
  </dt>
  <dd>
    <p>We have no solid information about good/bad/optimal values for this parameter.  Feedback with experience and observations are most welcome.</p>
  </dd>
  <dt>
    <p><strong>delayed</strong></p>
  </dt>
  <dd>
    <p>This parameter can be changed on the fly, but will not take effect immediately.</p>
  </dd>
  <dt>
    <p><strong>restart</strong></p>
  </dt>
  <dd>
    <p>The worker process must be stopped and restarted, before this parameter takes effect.</p>
  </dd>
  <dt>
    <p><strong>reload</strong></p>
  </dt>
  <dd>
    <p>The VCL programs must be reloaded for this parameter to take effect.</p>
  </dd>
  <dt>
    <p><strong>experimental</strong></p>
  </dt>
  <dd>
    <p>We&apos;re not really sure about this parameter, tell us what you find.</p>
  </dd>
  <dt>
    <p><strong>wizard</strong></p>
  </dt>
  <dd>
    <p>Do not touch unless you <em>really</em> know what you&apos;re doing.</p>
  </dd>
  <dt>
    <p><strong>only_root</strong></p>
  </dt>
  <dd>
    <p>Only works if varnishd is running as root.</p><p>Here is a list of all parameters, current as of last time we remembered to update the manual page.  This text is produced from the same text you will find in the CLI if you use the param.show command, so should there be a new parameter which is not listed here, you can find the description using the CLI commands.</p><p>Be aware that on 32 bit systems, certain default values, such as workspace_client (=16k), thread_pool_workspace (=16k), http_resp_size (=8k), http_req_size (=12k), gzip_stack_buffer (=4k) and thread_pool_stack (=64k) are reduced relative to the values listed here, in order to conserve VM space.</p>
  </dd>

</dl>

<h3>acceptor_sleep_decay</h3>
<ul>
<li><p>Default: 0.9</p></li><li><p>Minimum: 0</p></li><li><p>Maximum: 1</p></li><li><p>Flags: experimental</p><p>If we run out of resources, such as file descriptors or worker threads, the acceptor will sleep between accepts. This parameter (multiplicatively) reduce the sleep duration for each succesfull accept. (ie: 0.9 = reduce by 10%)</p></li>
</ul>
<h3>acceptor_sleep_incr</h3>
<ul>
<li><p>Units: s</p></li><li><p>Default: 0.001</p></li><li><p>Minimum: 0.000</p></li><li><p>Maximum: 1.000</p></li><li><p>Flags: experimental</p><p>If we run out of resources, such as file descriptors or worker threads, the acceptor will sleep between accepts. This parameter control how much longer we sleep, each time we fail to accept a new connection.</p></li>
</ul>
<h3>acceptor_sleep_max</h3>
<ul>
<li><p>Units: s</p></li><li><p>Default: 0.050</p></li><li><p>Minimum: 0.000</p></li><li><p>Maximum: 10.000</p></li><li><p>Flags: experimental</p><p>If we run out of resources, such as file descriptors or worker threads, the acceptor will sleep between accepts. This parameter limits how long it can sleep between attempts to accept new connections.</p></li>
</ul>
<h3>auto_restart</h3>
<ul>
<li><p>Units: bool</p></li><li><p>Default: on</p><p>Restart child process automatically if it dies.</p></li>
</ul>
<h3>ban_dups</h3>
<ul>
<li><p>Units: bool</p></li><li><p>Default: on</p><p>Elimited older identical bans when new bans are created.  This test is CPU intensive and scales with the number and complexity of active (non-Gone) bans.  If identical bans are frequent, the amount of CPU needed to actually test  the bans will be similarly reduced.</p></li>
</ul>
<h3>ban_lurker_age</h3>
<ul>
<li><p>Units: s</p></li><li><p>Default: 60.000</p></li><li><p>Minimum: 0.000</p><p>The ban lurker does not process bans until they are this old.  Right when a ban is added, the most frequently hit objects will get tested against it as part of object lookup.  This parameter prevents the ban-lurker from kicking in, until the rush is over.</p></li>
</ul>
<h3>ban_lurker_batch</h3>
<ul>
<li><p>Default: 1000</p></li><li><p>Minimum: 1</p><p>How many objects the ban lurker examines before taking a ban_lurker_sleep.  Use this to pace the ban lurker so it does not eat too much CPU.</p></li>
</ul>
<h3>ban_lurker_sleep</h3>
<ul>
<li><p>Units: s</p></li><li><p>Default: 0.010</p></li><li><p>Minimum: 0.000</p><p>The ban lurker thread sleeps between work batches, in order to not monopolize CPU power.  When nothing is done, it sleeps a fraction of a second before looking for new work to do. A value of zero disables the ban lurker.</p></li>
</ul>
<h3>between_bytes_timeout</h3>
<ul>
<li><p>Units: s</p></li><li><p>Default: 60.000</p></li><li><p>Minimum: 0.000</p><p>Default timeout between bytes when receiving data from backend. We only wait for this many seconds between bytes before giving up. A value of 0 means it will never time out. VCL can override this default value for each backend request and backend request. This parameter does not apply to pipe.</p></li>
</ul>
<h3>busyobj_worker_cache</h3>
<ul>
<li><p>Units: bool</p></li><li><p>Default: off</p><p>Cache free busyobj per worker thread. Disable this if you have very high hitrates and want to save the memory of one busyobj per worker thread.</p></li>
</ul>
<h3>cc_command</h3>
<ul>
<li><p>Default: "exec gcc -std=gnu99 -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wall -Werror -Wno-error=unused-result -pthread -fpic -shared -Wl,-x -o %o %s"</p></li><li><p>Flags: must_reload</p><p>Command used for compiling the C source code to a <a href="../man3/dlopen.3.html"><strong>dlopen</strong>(3)</a> loadable object.  Any occurrence of %s in the string will be replaced with the source file name, and %o will be replaced with the output file name.</p></li>
</ul>
<h3>cli_buffer</h3>
<ul>
<li><p>Units: bytes</p></li><li><p>Default: 8k</p></li><li><p>Minimum: 4k</p><p>Size of buffer for CLI command input. You may need to increase this if you have big VCL files and use the vcl.inline CLI command. NB: Must be specified with -p to have effect.</p></li>
</ul>
<h3>cli_limit</h3>
<ul>
<li><p>Units: bytes</p></li><li><p>Default: 48k</p></li><li><p>Minimum: 128b</p></li><li><p>Maximum: 99999999b</p><p>Maximum size of CLI response.  If the response exceeds this limit, the reponse code will be 201 instead of 200 and the last line will indicate the truncation.</p></li>
</ul>
<h3>cli_timeout</h3>
<ul>
<li><p>Units: seconds</p></li><li><p>Default: 60.000</p></li><li><p>Minimum: 0.000</p><p>Timeout for the childs replies to CLI requests from the mgt_param.</p></li>
</ul>
<h3>clock_skew</h3>
<ul>
<li><p>Units: s</p></li><li><p>Default: 10</p></li><li><p>Minimum: 0</p><p>How much clockskew we are willing to accept between the backend and our own clock.</p></li>
</ul>
<h3>connect_timeout</h3>
<ul>
<li><p>Units: s</p></li><li><p>Default: 3.500</p></li><li><p>Minimum: 0.000</p><p>Default connection timeout for backend connections. We only try to connect to the backend for this many seconds before giving up. VCL can override this default value for each backend and backend request.</p></li>
</ul>
<h3>critbit_cooloff</h3>
<ul>
<li><p>Units: s</p></li><li><p>Default: 180.000</p></li><li><p>Minimum: 60.000</p></li><li><p>Maximum: 254.000</p></li><li><p>Flags: wizard</p><p>How long time the critbit hasher keeps deleted objheads on the cooloff list.</p></li>
</ul>
<h3>debug</h3>
<ul>
<li><p>Default: none</p><p>Enable/Disable various kinds of debugging.</p></li>
</ul>
<dl class='dl-vertical'>
  <dt>
    <p><strong></strong><em>none</em><strong></strong></p>
  </dt>
  <dd>
    <p>Disable all debugging</p><p>Use +/- prefix to set/reset individual bits:</p>
  </dd>
  <dt>
    <p><strong></strong><em>req_state</em><strong></strong></p>
  </dt>
  <dd>
    <p>VSL Request state engine</p>
  </dd>
  <dt>
    <p><strong></strong><em>workspace</em><strong></strong></p>
  </dt>
  <dd>
    <p>VSL Workspace operations</p>
  </dd>
  <dt>
    <p><strong></strong><em>waiter</em><strong></strong></p>
  </dt>
  <dd>
    <p>VSL Waiter internals</p>
  </dd>
  <dt>
    <p><strong></strong><em>waitinglist</em><strong></strong></p>
  </dt>
  <dd>
    <p>VSL Waitinglist events</p>
  </dd>
  <dt>
    <p><strong></strong><em>syncvsl</em><strong></strong></p>
  </dt>
  <dd>
    <p>Make VSL synchronous</p>
  </dd>
  <dt>
    <p><strong></strong><em>hashedge</em><strong></strong></p>
  </dt>
  <dd>
    <p>Edge cases in Hash</p>
  </dd>
  <dt>
    <p><strong></strong><em>vclrel</em><strong></strong></p>
  </dt>
  <dd>
    <p>Rapid VCL release</p>
  </dd>
  <dt>
    <p><strong></strong><em>lurker</em><strong></strong></p>
  </dt>
  <dd>
    <p>VSL Ban lurker</p>
  </dd>
  <dt>
    <p><strong></strong><em>esi_chop</em><strong></strong></p>
  </dt>
  <dd>
    <p>Chop ESI fetch to bits</p>
  </dd>

</dl>

<h3>default_grace</h3>
<ul>
<li><p>Units: seconds</p></li><li><p>Default: 10.000</p></li><li><p>Minimum: 0.000</p></li><li><p>Flags:</p><p>Default grace period.  We will deliver an object this long after it has expired, provided another thread is attempting to get a new copy.</p></li>
</ul>
<h3>default_keep</h3>
<ul>
<li><p>Units: seconds</p></li><li><p>Default: 0.000</p></li><li><p>Minimum: 0.000</p></li><li><p>Flags:</p><p>Default keep period.  We will keep a useless object around this long, making it available for conditional backend fetches.  That means that the object will be removed from the cache at the end of ttl+grace+keep.</p></li>
</ul>
<h3>default_ttl</h3>
<ul>
<li><p>Units: seconds</p></li><li><p>Default: 120.000</p></li><li><p>Minimum: 0.000</p></li><li><p>Flags:</p><p>The TTL assigned to objects if neither the backend nor the VCL code assigns one.</p></li>
</ul>
<h3>feature</h3>
<ul>
<li><p>Default: none</p><p>Enable/Disable various minor features.</p></li>
</ul>
<dl class='dl-vertical'>
  <dt>
    <p><strong></strong><em>none</em><strong></strong></p>
  </dt>
  <dd>
    <p>Disable all features.</p><p>Use +/- prefix to enable/disable individual feature:</p>
  </dd>
  <dt>
    <p><strong></strong><em>short_panic</em><strong></strong></p>
  </dt>
  <dd>
    <p>Short panic message.</p>
  </dd>
  <dt>
    <p><strong></strong><em>wait_silo</em><strong></strong></p>
  </dt>
  <dd>
    <p>Wait for persistent silo.</p>
  </dd>
  <dt>
    <p><strong></strong><em>no_coredump</em><strong></strong></p>
  </dt>
  <dd>
    <p>No coredumps.</p>
  </dd>
  <dt>
    <p><strong></strong><em>esi_ignore_https</em><strong></strong></p>
  </dt>
  <dd>
    <p>Treat HTTPS as HTTP in ESI:includes</p>
  </dd>
  <dt>
    <p><strong></strong><em>esi_disable_xml_check</em><strong></strong></p>
  </dt>
  <dd>
    <p>Don&apos;t check of body looks like XML</p>
  </dd>
  <dt>
    <p><strong></strong><em>esi_ignore_other_elements</em><strong></strong></p>
  </dt>
  <dd>
    <p>Ignore non-esi XML-elements</p>
  </dd>
  <dt>
    <p><strong></strong><em>esi_remove_bom</em><strong></strong></p>
  </dt>
  <dd>
    <p>Remove UTF-8 BOM</p>
  </dd>

</dl>

<h3>fetch_chunksize</h3>
<ul>
<li><p>Units: bytes</p></li><li><p>Default: 128k</p></li><li><p>Minimum: 4k</p></li><li><p>Flags: experimental</p><p>The default chunksize used by fetcher. This should be bigger than the majority of objects with short TTLs. Internal limits in the storage_file module makes increases above 128kb a dubious idea.</p></li>
</ul>
<h3>fetch_maxchunksize</h3>
<ul>
<li><p>Units: bytes</p></li><li><p>Default: 0.25G</p></li><li><p>Minimum: 64k</p></li><li><p>Flags: experimental</p><p>The maximum chunksize we attempt to allocate from storage. Making this too large may cause delays and storage fragmentation.</p></li>
</ul>
<h3>first_byte_timeout</h3>
<ul>
<li><p>Units: s</p></li><li><p>Default: 60.000</p></li><li><p>Minimum: 0.000</p><p>Default timeout for receiving first byte from backend. We only wait for this many seconds for the first byte before giving up. A value of 0 means it will never time out. VCL can override this default value for each backend and backend request. This parameter does not apply to pipe.</p></li>
</ul>
<h3>group</h3>
<ul>
<li><p>Default: nogroup (65534)</p></li><li><p>Flags: must_restart, only_root</p><p>The unprivileged group to run as.</p></li>
</ul>
<h3>group_cc</h3>
<ul>
<li><p>Default: &lt;not set&gt;</p></li><li><p>Flags: only_root</p><p>On some systems the C-compiler is restricted so not everybody can run it.  This parameter makes it possible to add an extra group to the sandbox process which runs the cc_command, in order to gain access to such a restricted C-compiler.</p></li>
</ul>
<h3>gzip_buffer</h3>
<ul>
<li><p>Units: bytes</p></li><li><p>Default: 4k</p></li><li><p>Minimum: 2k</p></li><li><p>Flags: experimental</p><p>Size of malloc buffer used for gzip processing. These buffers are used for in-transit data, for instance gunzip&apos;ed data being sent to a client.Making this space to small results in more overhead, writes to sockets etc, making it too big is probably just a waste of memory.</p></li>
</ul>
<h3>gzip_level</h3>
<ul>
<li><p>Default: 6</p></li><li><p>Minimum: 0</p></li><li><p>Maximum: 9</p><p>Gzip compression level: 0=debug, 1=fast, 9=best</p></li>
</ul>
<h3>gzip_memlevel</h3>
<ul>
<li><p>Default: 8</p></li><li><p>Minimum: 1</p></li><li><p>Maximum: 9</p><p>Gzip memory level 1=slow/least, 9=fast/most compression. Memory impact is 1=1k, 2=2k, ... 9=256k.</p></li>
</ul>
<h3>http_gzip_support</h3>
<ul>
<li><p>Units: bool</p></li><li><p>Default: on</p></li>
</ul>
<dl class='dl-vertical'>
  <dt>
    <p><strong>Enable gzip support. When enabled Varnish request compressed objects from the backend and store them compressed. If a client does not support gzip encoding Varnish will uncompress compressed objects on demand. Varnish will also rewrite the Accept-Encoding header of clients indicating support for gzip to:</strong></p>
  </dt>
  <dd>
    <p>Accept-Encoding: gzip</p><p>Clients that do not support gzip will have their Accept-Encoding header removed. For more information on how gzip is implemented please see the chapter on gzip in the Varnish reference.</p>
  </dd>

</dl>

<h3>http_max_hdr</h3>
<ul>
<li><p>Units: header lines</p></li><li><p>Default: 64</p></li><li><p>Minimum: 32</p></li><li><p>Maximum: 65535</p><p>Maximum number of HTTP header lines we allow in {req|resp|bereq|beresp}.http (obj.http is autosized to the exact number of headers). Cheap, ~20 bytes, in terms of workspace memory. Note that the first line occupies five header lines.</p></li>
</ul>
<h3>http_range_support</h3>
<ul>
<li><p>Units: bool</p></li><li><p>Default: on</p><p>Enable support for HTTP Range headers.</p></li>
</ul>
<h3>http_req_hdr_len</h3>
<ul>
<li><p>Units: bytes</p></li><li><p>Default: 8k</p></li><li><p>Minimum: 40b</p><p>Maximum length of any HTTP client request header we will allow.  The limit is inclusive its continuation lines.</p></li>
</ul>
<h3>http_req_size</h3>
<ul>
<li><p>Units: bytes</p></li><li><p>Default: 12k</p></li><li><p>Minimum: 0.25k</p><p>Maximum number of bytes of HTTP client request we will deal with.  This is a limit on all bytes up to the double blank line which ends the HTTP request. The memory for the request is allocated from the client workspace (param: workspace_client) and this parameter limits how much of that the request is allowed to take up.</p></li>
</ul>
<h3>http_resp_hdr_len</h3>
<ul>
<li><p>Units: bytes</p></li><li><p>Default: 8k</p></li><li><p>Minimum: 40b</p><p>Maximum length of any HTTP backend response header we will allow.  The limit is inclusive its continuation lines.</p></li>
</ul>
<h3>http_resp_size</h3>
<ul>
<li><p>Units: bytes</p></li><li><p>Default: 8k</p></li><li><p>Minimum: 0.25k</p><p>Maximum number of bytes of HTTP backend resonse we will deal with.  This is a limit on all bytes up to the double blank line which ends the HTTP request. The memory for the request is allocated from the worker workspace (param: thread_pool_workspace) and this parameter limits how much of that the request is allowed to take up.</p></li>
</ul>
<h3>idle_send_timeout</h3>
<ul>
<li><p>Units: seconds</p></li><li><p>Default: 60.000</p></li><li><p>Minimum: 0.000</p></li><li><p>Flags: delayed</p><p>Time to wait with no data sent. If no data has been transmitted in this many seconds the session is closed. See <a href="../man2/setsockopt.2.html"><strong>setsockopt</strong>(2)</a> under SO_SNDTIMEO for more information.</p></li>
</ul>
<h3>listen_address</h3>
<ul>
<li><p>Default: :80</p></li><li><p>Flags: must_restart</p><p>Whitespace separated list of network endpoints where Varnish will accept requests. Possible formats: host, host:port, :port</p></li>
</ul>
<h3>listen_depth</h3>
<ul>
<li><p>Units: connections</p></li><li><p>Default: 1024</p></li><li><p>Minimum: 0</p></li><li><p>Flags: must_restart</p><p>Listen queue depth.</p></li>
</ul>
<h3>lru_interval</h3>
<ul>
<li><p>Units: seconds</p></li><li><p>Default: 2.000</p></li><li><p>Minimum: 0.000</p></li><li><p>Flags: experimental</p><p>Grace period before object moves on LRU list. Objects are only moved to the front of the LRU list if they have not been moved there already inside this timeout period.  This reduces the amount of lock operations necessary for LRU list access.</p></li>
</ul>
<h3>max_esi_depth</h3>
<ul>
<li><p>Units: levels</p></li><li><p>Default: 5</p></li><li><p>Minimum: 0</p><p>Maximum depth of esi:include processing.</p></li>
</ul>
<h3>max_restarts</h3>
<ul>
<li><p>Units: restarts</p></li><li><p>Default: 4</p></li><li><p>Minimum: 0</p><p>Upper limit on how many times a request can restart. Be aware that restarts are likely to cause a hit against the backend, so don&apos;t increase thoughtlessly.</p></li>
</ul>
<h3>max_retries</h3>
<ul>
<li><p>Units: retries</p></li><li><p>Default: 4</p></li><li><p>Minimum: 0</p><p>Upper limit on how many times a backend fetch can retry.</p></li>
</ul>
<h3>nuke_limit</h3>
<ul>
<li><p>Units: allocations</p></li><li><p>Default: 50</p></li><li><p>Minimum: 0</p></li><li><p>Flags: experimental</p><p>Maximum number of objects we attempt to nuke in orderto make space for a object body.</p></li>
</ul>
<h3>pcre_match_limit</h3>
<ul>
<li><p>Default: 10000</p></li><li><p>Minimum: 1</p><p>The limit for the  number of internal matching function calls in a pcre_exec() execution.</p></li>
</ul>
<h3>pcre_match_limit_recursion</h3>
<ul>
<li><p>Default: 10000</p></li><li><p>Minimum: 1</p><p>The limit for the  number of internal matching function recursions in a pcre_exec() execution.</p></li>
</ul>
<h3>ping_interval</h3>
<ul>
<li><p>Units: seconds</p></li><li><p>Default: 3</p></li><li><p>Minimum: 0</p></li><li><p>Flags: must_restart</p><p>Interval between pings from parent to child. Zero will disable pinging entirely, which makes it possible to attach a debugger to the child.</p></li>
</ul>
<h3>pipe_timeout</h3>
<ul>
<li><p>Units: seconds</p></li><li><p>Default: 60.000</p></li><li><p>Minimum: 0.000</p><p>Idle timeout for PIPE sessions. If nothing have been received in either direction for this many seconds, the session is closed.</p></li>
</ul>
<h3>pool_req</h3>
<ul>
<li><p>Default: 10,100,10</p><p>Parameters for per worker pool request memory pool. The three numbers are:</p></li>
</ul>
<dl class='dl-vertical'>
  <dt>
    <p><strong></strong><em>min_pool</em><strong></strong></p>
  </dt>
  <dd>
    <p>minimum size of free pool.</p>
  </dd>
  <dt>
    <p><strong></strong><em>max_pool</em><strong></strong></p>
  </dt>
  <dd>
    <p>maximum size of free pool.</p>
  </dd>
  <dt>
    <p><strong></strong><em>max_age</em><strong></strong></p>
  </dt>
  <dd>
    <p>max age of free element.</p>
  </dd>

</dl>

<h3>pool_sess</h3>
<ul>
<li><p>Default: 10,100,10</p><p>Parameters for per worker pool session memory pool. The three numbers are:</p></li>
</ul>
<dl class='dl-vertical'>
  <dt>
    <p><strong></strong><em>min_pool</em><strong></strong></p>
  </dt>
  <dd>
    <p>minimum size of free pool.</p>
  </dd>
  <dt>
    <p><strong></strong><em>max_pool</em><strong></strong></p>
  </dt>
  <dd>
    <p>maximum size of free pool.</p>
  </dd>
  <dt>
    <p><strong></strong><em>max_age</em><strong></strong></p>
  </dt>
  <dd>
    <p>max age of free element.</p>
  </dd>

</dl>

<h3>pool_vbc</h3>
<ul>
<li><p>Default: 10,100,10</p><p>Parameters for backend connection memory pool. The three numbers are:</p></li>
</ul>
<dl class='dl-vertical'>
  <dt>
    <p><strong></strong><em>min_pool</em><strong></strong></p>
  </dt>
  <dd>
    <p>minimum size of free pool.</p>
  </dd>
  <dt>
    <p><strong></strong><em>max_pool</em><strong></strong></p>
  </dt>
  <dd>
    <p>maximum size of free pool.</p>
  </dd>
  <dt>
    <p><strong></strong><em>max_age</em><strong></strong></p>
  </dt>
  <dd>
    <p>max age of free element.</p>
  </dd>

</dl>

<h3>pool_vbo</h3>
<ul>
<li><p>Default: 10,100,10</p><p>Parameters for backend object fetch memory pool. The three numbers are:</p></li>
</ul>
<dl class='dl-vertical'>
  <dt>
    <p><strong></strong><em>min_pool</em><strong></strong></p>
  </dt>
  <dd>
    <p>minimum size of free pool.</p>
  </dd>
  <dt>
    <p><strong></strong><em>max_pool</em><strong></strong></p>
  </dt>
  <dd>
    <p>maximum size of free pool.</p>
  </dd>
  <dt>
    <p><strong></strong><em>max_age</em><strong></strong></p>
  </dt>
  <dd>
    <p>max age of free element.</p>
  </dd>

</dl>

<h3>prefer_ipv6</h3>
<ul>
<li><p>Units: bool</p></li><li><p>Default: off</p><p>Prefer IPv6 address when connecting to backends which have both IPv4 and IPv6 addresses.</p></li>
</ul>
<h3>rush_exponent</h3>
<ul>
<li><p>Units: requests per request</p></li><li><p>Default: 3</p></li><li><p>Minimum: 2</p></li><li><p>Flags: experimental</p><p>How many parked request we start for each completed request on the object. NB: Even with the implict delay of delivery, this parameter controls an exponential increase in number of worker threads.</p></li>
</ul>
<h3>send_timeout</h3>
<ul>
<li><p>Units: seconds</p></li><li><p>Default: 600.000</p></li><li><p>Minimum: 0.000</p></li><li><p>Flags: delayed</p><p>Send timeout for client connections. If the HTTP response hasn&apos;t been transmitted in this many seconds the session is closed. See <a href="../man2/setsockopt.2.html"><strong>setsockopt</strong>(2)</a> under SO_SNDTIMEO for more information.</p></li>
</ul>
<h3>session_max</h3>
<ul>
<li><p>Units: sessions</p></li><li><p>Default: 100000</p></li><li><p>Minimum: 1000</p><p>Maximum number of sessions we will allocate from one pool before just dropping connections. This is mostly an anti-DoS measure, and setting it plenty high should not hurt, as long as you have the memory for it.</p></li>
</ul>
<h3>shm_reclen</h3>
<ul>
<li><p>Units: bytes</p></li><li><p>Default: 255b</p></li><li><p>Minimum: 16b</p></li><li><p>Maximum: 4084</p><p>Old name for vsl_reclen, use that instead.</p></li>
</ul>
<h3>shortlived</h3>
<ul>
<li><p>Units: s</p></li><li><p>Default: 10.000</p></li><li><p>Minimum: 0.000</p><p>Objects created with (ttl+grace+keep) shorter than this are always put in transient storage.</p></li>
</ul>
<h3>sigsegv_handler</h3>
<ul>
<li><p>Units: bool</p></li><li><p>Default: off</p></li><li><p>Flags: must_restart</p><p>Install a signal handler which tries to dump debug information on segmentation faults.</p></li>
</ul>
<h3>syslog_cli_traffic</h3>
<ul>
<li><p>Units: bool</p></li><li><p>Default: on</p><p>Log all CLI traffic to syslog(LOG_INFO).</p></li>
</ul>
<h3>tcp_keepalive_intvl</h3>
<ul>
<li><p>Units: seconds</p></li><li><p>Default: 75.000</p></li><li><p>Minimum: 1.000</p></li><li><p>Maximum: 100.000</p></li><li><p>Flags: experimental</p><p>The number of seconds between TCP keep-alive probes.</p></li>
</ul>
<h3>tcp_keepalive_probes</h3>
<ul>
<li><p>Units: probes</p></li><li><p>Default: 9</p></li><li><p>Minimum: 1</p></li><li><p>Maximum: 100</p></li><li><p>Flags: experimental</p><p>The maximum number of TCP keep-alive probes to send before giving up and killing the connection if no response is obtained from the other end.</p></li>
</ul>
<h3>tcp_keepalive_time</h3>
<ul>
<li><p>Units: seconds</p></li><li><p>Default: 7200.000</p></li><li><p>Minimum: 1.000</p></li><li><p>Maximum: 7200.000</p></li><li><p>Flags: experimental</p><p>The number of seconds a connection needs to be idle before TCP begins sending out keep-alive probes.</p></li>
</ul>
<h3>thread_pool_add_delay</h3>
<ul>
<li><p>Units: seconds</p></li><li><p>Default: 0.000</p></li><li><p>Minimum: 0.000</p></li><li><p>Flags: experimental</p><p>Wait at least this long after creating a thread.</p><p>Some (buggy) systems may need a short (sub-second) delay between creating threads. Set this to a few milliseconds if you see the &apos;threads_failed&apos; counter grow too much.</p><p>Setting this too high results in insuffient worker threads.</p></li>
</ul>
<h3>thread_pool_destroy_delay</h3>
<ul>
<li><p>Units: seconds</p></li><li><p>Default: 1.000</p></li><li><p>Minimum: 0.010</p></li><li><p>Flags: delayed, experimental</p><p>Wait this long after destroying a thread.</p><p>This controls the decay of thread pools when idle(-ish).</p><p>Minimum is 0.01 second.</p></li>
</ul>
<h3>thread_pool_fail_delay</h3>
<ul>
<li><p>Units: seconds</p></li><li><p>Default: 0.200</p></li><li><p>Minimum: 0.010</p></li><li><p>Flags: experimental</p><p>Wait at least this long after a failed thread creation before trying to create another thread.</p><p>Failure to create a worker thread is often a sign that  the end is near, because the process is running out of some resource.  This delay tries to not rush the end on needlessly.</p><p>If thread creation failures are a problem, check that thread_pool_max is not too high.</p><p>It may also help to increase thread_pool_timeout and thread_pool_min, to reduce the rate at which treads are destroyed and later recreated.</p></li>
</ul>
<h3>thread_pool_max</h3>
<ul>
<li><p>Units: threads</p></li><li><p>Default: 5000</p></li><li><p>Minimum: 100</p></li><li><p>Flags: delayed</p><p>The maximum number of worker threads in each pool.</p><p>Do not set this higher than you have to, since excess worker threads soak up RAM and CPU and generally just get in the way of getting work done.</p><p>Minimum is 10 threads.</p></li>
</ul>
<h3>thread_pool_min</h3>
<ul>
<li><p>Units: threads</p></li><li><p>Default: 100</p></li><li><p>Maximum: 5000</p></li><li><p>Flags: delayed</p><p>The minimum number of worker threads in each pool.</p><p>Increasing this may help ramp up faster from low load situations or when threads have expired.</p><p>Minimum is 10 threads.</p></li>
</ul>
<h3>thread_pool_stack</h3>
<ul>
<li><p>Units: bytes</p></li><li><p>Default: 48k</p></li><li><p>Minimum: 16k</p></li><li><p>Flags: experimental</p><p>Worker thread stack size. This will likely be rounded up to a multiple of 4k (or whatever the page_size might be) by the kernel.</p></li>
</ul>
<h3>thread_pool_timeout</h3>
<ul>
<li><p>Units: seconds</p></li><li><p>Default: 300.000</p></li><li><p>Minimum: 10.000</p></li><li><p>Flags: delayed, experimental</p><p>Thread idle threshold.</p><p>Threads in excess of thread_pool_min, which have been idle for at least this long, will be destroyed.</p><p>Minimum is 10 seconds.</p></li>
</ul>
<h3>thread_pools</h3>
<ul>
<li><p>Units: pools</p></li><li><p>Default: 2</p></li><li><p>Minimum: 1</p></li><li><p>Flags: delayed, experimental</p><p>Number of worker thread pools.</p><p>Increasing number of worker pools decreases lock contention.</p><p>Too many pools waste CPU and RAM resources, and more than one pool for each CPU is probably detrimal to performance.</p><p>Can be increased on the fly, but decreases require a restart to take effect.</p></li>
</ul>
<h3>thread_queue_limit</h3>
<ul>
<li><p>Default: 20</p></li><li><p>Minimum: 0</p></li><li><p>Flags: experimental</p><p>Permitted queue length per thread-pool.</p><p>This sets the number of requests we will queue, waiting for an available thread.  Above this limit sessions will be dropped instead of queued.</p></li>
</ul>
<h3>thread_stats_rate</h3>
<ul>
<li><p>Units: requests</p></li><li><p>Default: 10</p></li><li><p>Minimum: 0</p></li><li><p>Flags: experimental</p><p>Worker threads accumulate statistics, and dump these into the global stats counters if the lock is free when they finish a job (request/fetch etc.) This parameters defines the maximum number of jobs a worker thread may handle, before it is forced to dump its accumulated stats into the global counters.</p></li>
</ul>
<h3>timeout_idle</h3>
<ul>
<li><p>Units: seconds</p></li><li><p>Default: 5.000</p></li><li><p>Minimum: 0.000</p><p>Idle timeout for client connections. A connection is considered idle, until we receive a non-white-space character on it.</p></li>
</ul>
<h3>timeout_linger</h3>
<ul>
<li><p>Units: seconds</p></li><li><p>Default: 0.050</p></li><li><p>Minimum: 0.000</p></li><li><p>Flags: experimental</p><p>How long time the workerthread lingers on an idle session before handing it over to the waiter. When sessions are reused, as much as half of all reuses happen within the first 100 msec of the previous request completing. Setting this too high results in worker threads not doing anything for their keep, setting it too low just means that more sessions take a detour around the waiter.</p></li>
</ul>
<h3>timeout_req</h3>
<ul>
<li><p>Units: seconds</p></li><li><p>Default: 2.000</p></li><li><p>Minimum: 0.000</p><p>Max time to receive clients request header, measured from first non-white-space character to double CRNL.</p></li>
</ul>
<h3>user</h3>
<ul>
<li><p>Default: nobody (65534)</p></li><li><p>Flags: must_restart, only_root</p><p>The unprivileged user to run as.</p></li>
</ul>
<h3>vcc_allow_inline_c</h3>
<ul>
<li><p>Units: bool</p></li><li><p>Default: off</p><p>Allow inline C code in VCL.</p></li>
</ul>
<h3>vcc_err_unref</h3>
<ul>
<li><p>Units: bool</p></li><li><p>Default: on</p><p>Unreferenced VCL objects result in error.</p></li>
</ul>
<h3>vcc_unsafe_path</h3>
<ul>
<li><p>Units: bool</p></li><li><p>Default: on</p><p>Allow &apos;/&apos; in vmod & include paths. Allow &apos;import ... from ...&apos;.</p></li>
</ul>
<h3>vcl_dir</h3>
<ul>
<li><p>Default: /etc/varnish</p><p>Directory from which relative VCL filenames (vcl.load and include) are opened.</p></li>
</ul>
<h3>vmod_dir</h3>
<ul>
<li><p>Default: /usr/lib/i386-linux-gnu/varnish/vmods</p><p>Directory where VCL modules are to be found.</p></li>
</ul>
<h3>vsl_buffer</h3>
<ul>
<li><p>Units: bytes</p></li><li><p>Default: 4k</p></li><li><p>Minimum: 267</p><p>Bytes of (req-/backend-)workspace dedicated to buffering VSL records. Setting this too high costs memory, setting it too low will cause more VSL flushes and likely increase lock-contention on the VSL mutex.</p><p>The minimum tracks the vsl_reclen parameter + 12 bytes.</p></li>
</ul>
<h3>vsl_mask</h3>
<ul>
<li><p>Default: -VCL_trace,-WorkThread,-Hash</p><p>Mask individual VSL messages from being logged.</p></li>
</ul>
<dl class='dl-vertical'>
  <dt>
    <p><strong></strong><em>default</em><strong></strong></p>
  </dt>
  <dd>
    <p>Set default value</p><p>Use +/- prefixe in front of VSL tag name, to mask/unmask individual VSL messages.</p>
  </dd>

</dl>

<h3>vsl_reclen</h3>
<ul>
<li><p>Units: bytes</p></li><li><p>Default: 255b</p></li><li><p>Minimum: 16b</p></li><li><p>Maximum: 4084b</p><p>Maximum number of bytes in SHM log record.</p><p>The maximum tracks the vsl_buffer parameter - 12 bytes.</p></li>
</ul>
<h3>vsl_space</h3>
<ul>
<li><p>Units: bytes</p></li><li><p>Default: 80M</p></li><li><p>Minimum: 1M</p></li><li><p>Flags: must_restart</p><p>The amount of space to allocate for the VSL fifo buffer in the VSM memory segment.  If you make this too small, varnish{ncsa|log} etc will not be able to keep up.  Making it too large just costs memory resources.</p></li>
</ul>
<h3>vsm_space</h3>
<ul>
<li><p>Units: bytes</p></li><li><p>Default: 1M</p></li><li><p>Minimum: 1M</p></li><li><p>Flags: must_restart</p><p>The amount of space to allocate for stats counters in the VSM memory segment.  If you make this too small, some counters will be invisible.  Making it too large just costs memory resources.</p></li>
</ul>
<h3>waiter</h3>
<ul>
<li><p>Default: epoll (possible values: epoll, poll)</p></li><li><p>Flags: must_restart, wizard</p><p>Select the waiter kernel interface.</p></li>
</ul>
<h3>workspace_backend</h3>
<ul>
<li><p>Units: bytes</p></li><li><p>Default: 16k</p></li><li><p>Minimum: 1k</p></li><li><p>Flags: delayed</p><p>Bytes of HTTP protocol workspace for backend HTTP req/resp.  If larger than 4k, use a multiple of 4k for VM efficiency.</p></li>
</ul>
<h3>workspace_client</h3>
<ul>
<li><p>Units: bytes</p></li><li><p>Default: 24k</p></li><li><p>Minimum: 9k</p></li><li><p>Flags: delayed</p><p>Bytes of HTTP protocol workspace for clients HTTP req/resp.  If larger than 4k, use a multiple of 4k for VM efficiency.</p></li>
</ul>
<h3>workspace_session</h3>
<ul>
<li><p>Units: bytes</p></li><li><p>Default: 384b</p></li><li><p>Minimum: 0.25k</p></li><li><p>Flags: delayed</p><p>Bytes of workspace for session and TCP connection addresses.  If larger than 4k, use a multiple of 4k for VM efficiency.</p></li>
</ul>
<h3>workspace_thread</h3>
<ul>
<li><p>Units: bytes</p></li><li><p>Default: 2k</p></li><li><p>Minimum: 0.25k</p></li><li><p>Maximum: 8k</p></li><li><p>Flags: delayed</p><p>Bytes of auxillary workspace per thread. This workspace is used for certain temporary data structures during the operation of a worker thread. One use is for the io-vectors for writing requests and responses to sockets, having too little space will result in more <a href="../man2/writev.2.html"><strong>writev</strong>(2)</a> system calls, having too much just wastes the space.</p></li>
</ul>

        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">EXIT CODES</h2>
        <div class="sectioncontent">
<p>Varnish and bundled tools will, in most cases, exit with one of the following codes</p><ul>
<li><p><em>0</em> OK</p></li><li><p><em>1</em> Some error which could be system-dependend and/or transient</p></li><li><p><em>2</em> Serious configuration / parameter error - retrying with the same configuration / parameters is most likely useless</p><p>The <em>varnishd</em> master process may also OR its exit code</p></li><li><p>with <em>0x20</em> when the <em>varnishd</em> child process died,</p></li><li><p>with <em>0x40</em> when the <em>varnishd</em> child process was terminated by a signal and</p></li><li><p>with <em>0x80</em> when a core was dumped.</p></li>
</ul>
        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">RELATED TO varnishd&hellip;</h2>
        <div class="sectioncontent">
<ul>
<li><p><strong>varnish-cli</strong>(7)</p></li><li><p><a href="../man1/varnishlog.1.html"><strong>varnishlog</strong>(1)</a></p></li><li><p><a href="../man1/varnishhist.1.html"><strong>varnishhist</strong>(1)</a></p></li><li><p><a href="../man1/varnishncsa.1.html"><strong>varnishncsa</strong>(1)</a></p></li><li><p><strong>varnishstat</strong>(1)</p></li><li><p><a href="../man1/varnishtop.1.html"><strong>varnishtop</strong>(1)</a></p></li><li><p><strong>vcl</strong>(7)</p></li>
</ul>
        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">HISTORY</h2>
        <div class="sectioncontent">
<p>The varnishd daemon was developed by Poul-Henning Kamp in cooperation with Verdens Gang AS and Varnish Software.</p><p>This manual page was written by Dag-Erling Smørgrav with updates by Stig Sandbeck Mathisen &lt;<em>ssm@debian.org</em>&gt;.</p>
        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">COPYRIGHT</h2>
        <div class="sectioncontent">
<p>This document is licensed under the same licence as Varnish itself. See LICENCE for details.</p><ul>
<li><p>Copyright (c) 2007-2014 Varnish Software AS</p></li>
</ul>
        </div>
      </section>
<nav>
  <ul class="pager">
   <li class="previous"><a href="vapigen-0.26.1.html"><span aria-hidden="true">&larr;</span> vapigen-0.26.1: Generate a vala api</a></li>
   <li class="next"><a href="varnishhist.1.html">varnishhist.1: Varnish request histogram <span aria-hidden="true">&rarr;</span></a></li>
  </ul>
</nav>

  </div>
  <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
  <!-- Include all compiled plugins (below), or include individual files as needed -->
  <script src="/js/bootstrap.min.js"></script>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-60781387-1', 'auto');
    ga('send', 'pageview');

  </script>
</body>
</html>
