<!DOCTYPE html>
<html lang="en">
<head>
  <link href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700&effect=destruction%7Cshadow-multiple" rel="stylesheet" type="text/css">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>pgloader: Postgresql data loader</title>
  <link href="/css/bootstrap.min.css" rel="stylesheet">
  <link href="/css/manpage.css" rel="stylesheet">
  <link rel="apple-touch-icon" sizes="57x57" href="/icons/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/icons/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/icons/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/icons/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/icons/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/icons/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/icons/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/icons/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192" href="/icons/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/icons/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png">
  <link rel="manifest" href="/icons/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/icons/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">
  <meta name="description" content="Postgresql data loader">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@CartaTech">
  <meta name="twitter:creator" content="@CartaTech">
  <meta name="twitter:title" content="pgloader (1) manual">
  <meta name="twitter:description" content="Postgresql data loader">
  <meta name="twitter:image" content="https://www.carta.tech/images/pgloader-pgloader-1.png">
  <meta property="og:url" content="https://www.carta.tech/man-pages/man1/pgloader.1.html" />
  <meta property="og:type" content="website" />
  <meta property="og:title" content="pgloader (1) manual" />
  <meta property="og:description" content="Postgresql data loader" />
  <meta property="fb:app_id" content="1241677679199500" />
  <meta property="og:image" content="https://www.carta.tech/images/pgloader-pgloader-1.png" />
  <meta property="og:image:width" content="600" />
  <meta property="og:image:height" content="315" />
</head>
<body>
  <div class="container final">
          <div class="page-header">
        <h1 class="font-effect-destruction">pgloader<small> (1)</small></h1>
        <p class="lead">Postgresql data loader</p>
      </div>

    <ol class="breadcrumb" itemscope itemtype="http://schema.org/BreadcrumbList">
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/">
      <span itemprop="name">Carta.tech</span>
    </a>
    <meta itemprop="position" content="1" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/man-pages/">
      <span itemprop="name">Man Pages</span>
    </a>
    <meta itemprop="position" content="2" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/man-pages/man1/">
      <span itemprop="name">Executable programs or shell commands</span>
    </a>
    <meta itemprop="position" content="3" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/man-pages/man1/pgloader.1.html">
      <span itemprop="name">pgloader: Postgresql data loader</span>
    </a>
    <meta itemprop="position" content="4" />
  </li>
</ol>
<ol class="breadcrumb" itemscope itemtype="http://schema.org/BreadcrumbList">
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/">
      <span itemprop="name">Carta.tech</span>
    </a>
    <meta itemprop="position" content="1" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/packages/">
      <span itemprop="name">Packages</span>
    </a>
    <meta itemprop="position" content="2" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/packages/pgloader/">
      <span itemprop="name">pgloader</span>
    </a>
    <meta itemprop="position" content="3" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/man-pages/man1/pgloader.1.html">
      <span itemprop="name">pgloader: Postgresql data loader</span>
    </a>
    <meta itemprop="position" content="4" />
  </li>
</ol>
    
      <section>
        <h2 class="font-effect-shadow-multiple">SYNOPSIS</h2>
        <div class="sectioncontent">
<p><strong>pgloader</strong> [<em>options</em>] [<em>command-file</em>]...</p>
        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">DESCRIPTION</h2>
        <div class="sectioncontent">
<p>pgloader loads data from various sources into PostgreSQL. It can transform the data it reads on the fly and submit raw SQL before and after the loading. It uses the <strong>COPY</strong> PostgreSQL protocol to stream the data into the server, and manages errors by filling a pair of <em>reject.dat</em> and <em>reject.log</em> files.</p><p>pgloader operates using commands which are read from files:</p><ul>
<li>
<pre>
pgloader commands.load
</pre>
</li><li></li>
</ul>
        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">OPTIONS</h2>
        <div class="sectioncontent">

<dl class='dl-vertical'>
  <dt>
    <p><strong>-h</strong>, <strong>--help</strong></p>
  </dt>
  <dd>
    <p>Show command usage summary and exit.</p>
  </dd>
  <dt>
    <p><strong>-V</strong>, <strong>--version</strong></p>
  </dt>
  <dd>
    <p>Show pgloader version string and exit.</p>
  </dd>
  <dt>
    <p><strong>-v</strong>, <strong>--verbose</strong></p>
  </dt>
  <dd>
    <p>Be verbose.</p>
  </dd>
  <dt>
    <p><strong>-q</strong>, `--quiet</p>
  </dt>
  <dd>
    <p>Be quiet.</p>
  </dd>
  <dt>
    <p><strong>-d</strong>, <strong>--debug</strong></p>
  </dt>
  <dd>
    <p>Show debug level information messages.</p>
  </dd>
  <dt>
    <p><strong>-D</strong>, <strong>--root-dir</strong></p>
  </dt>
  <dd>
    <p>Set the root working directory (default to "/tmp/pgloader").</p>
  </dd>
  <dt>
    <p><strong>-L</strong>, <strong>--logfile</strong></p>
  </dt>
  <dd>
    <p>Set the pgloader log file (default to "/tmp/pgloader.log").</p>
  </dd>
  <dt>
    <p><strong>--log-min-messages</strong></p>
  </dt>
  <dd>
    <p>Minimum level of verbosity needed for log message to make it to the logfile. One of critical, log, error, warning, notice, info or debug.</p>
  </dd>
  <dt>
    <p><strong>--client-min-messages</strong></p>
  </dt>
  <dd>
    <p>Minimum level of verbosity needed for log message to make it to the console. One of critical, log, error, warning, notice, info or debug.</p>
  </dd>
  <dt>
    <p><strong>-S</strong>, <strong>--summary</strong></p>
  </dt>
  <dd>
    <p>A filename where to copy the summary output. When relative, the filename is expanded into <strong>*root-dir</strong>.</p>
  </dd>
  <dt>
    <p><strong>-E</strong>, <strong>--list-encodings</strong></p>
  </dt>
  <dd>
    <p>List known encodings in this version of pgloader.</p>
  </dd>
  <dt>
    <p><strong>-U</strong>, <strong>--upgrade-config</strong></p>
  </dt>
  <dd>
    <p>Parse given files in the command line as <strong>pgloader.conf</strong> files with the <strong>INI</strong> syntax that was in use in pgloader versions 2.x, and output the new command syntax for pgloader on standard output.</p>
  </dd>
  <dt>
    <p><strong>-l &lt;file&gt;</strong>, <strong>--load-lisp-file &lt;file&gt;</strong></p>
  </dt>
  <dd>
    <p>Specify a lisp <em>file</em> to compile and load into the pgloader image before reading the commands, allowing to define extra transformation function. Those functions should be defined in the <strong>pgloader.transforms</strong> package. This option can appear more than once in the command line.</p>
  </dd>
  <dt>
    <p><strong>--self-upgrade &lt;directory&gt;</strong>:</p>
  </dt>
  <dd>
    <ul>
<li><p>Specify a <em>directory</em> where to find pgloader sources so that one of the very first things it does is dynamically loading-in (and compiling to machine code) another version of itself, usually a newer one like a very recent git checkout.</p></li>
</ul>
  </dd>

</dl>
<p>To get the maximum amount of debug information, you can use both the <strong>--verbose</strong> and the <strong>--debug</strong> switches at the same time, which is equivalent to saying <strong>--client-min-messages data</strong>. Then the log messages will show the data being processed, in the cases where the code has explicit support for it.</p>
        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">BATCHES AND RETRY BEHAVIOUR</h2>
        <div class="sectioncontent">
<p>To load data to PostgreSQL, pgloader uses the <strong>COPY</strong> streaming protocol. While this is the faster way to load data, <strong>COPY</strong> has an important drawback: as soon as PostgreSQL emits an error with any bit of data sent to it, whatever the problem is, the whole data set is rejected by PostgreSQL.</p><p>To work around that, pgloader cuts the data into <em>batches</em> of 25000 rows each, so that when a problem occurs it\'s only impacting that many rows of data. Each batch is kept in memory while the <strong>COPY</strong> streaming happens, in order to be able to handle errors should some happen.</p><p>When PostgreSQL rejects the whole batch, pgloader logs the error message then isolates the bad row(s) from the accepted ones by retrying the batched rows in smaller batches. To do that, pgloader parses the <em>CONTEXT</em> error message from the failed COPY, as the message contains the line number where the error was found in the batch, as in the following example:</p><ul>
<li>
<pre>
CONTEXT: COPY errors, line 3, column b: "2006-13-11"
</pre>
</li><li></li>
</ul><p>Using that information, pgloader will reload all rows in the batch before the erroneous one, log the erroneous one as rejected, then try loading the remaining of the batch in a single attempt, which may or may not contain other erroneous data.</p><p>At the end of a load containing rejected rows, you will find two files in the <em>root-dir</em> location, under a directory named the same as the target database of your setup. The filenames are the target table, and their extensions are <strong>.dat</strong> for the rejected data and <strong>.log</strong> for the file containing the full PostgreSQL client side logs about the rejected data.</p><p>The <strong>.dat</strong> file is formatted in PostgreSQL the text COPY format as documented in http://www.postgresql.org/docs/9.2/static/sql-copy.html#AEN66609 <em></em>.</p>
        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">A NOTE ABOUT PERFORMANCES</h2>
        <div class="sectioncontent">
<p>pgloader has been developed with performances in mind, to be able to cope with ever growing needs in loading large amounts of data into PostgreSQL.</p><p>The basic architecture it uses is the old Unix pipe model, where a thread is responsible for loading the data (reading a CSV file, querying MySQL, etc) and fills pre-processed data into a queue. Another threads feeds from the queue, apply some more <em>transformations</em> to the input data and stream the end result to PostgreSQL using the COPY protocol.</p><p>When given a file that the PostgreSQL <strong>COPY</strong> command knows how to parse, and if the file contains no erroneous data, then pgloader will never be as fast as just using the PostgreSQL <strong>COPY</strong> command.</p><p>Note that while the <strong>COPY</strong> command is restricted to read either from its standard input or from a local file on the server\'s file system, the command line tool <strong>psql</strong> implements a <strong>&#92;copy</strong> command that knows how to stream a file local to the client over the network and into the PostgreSQL server, using the same protocol as pgloader uses.</p>
        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">COMMANDS</h2>
        <div class="sectioncontent">
<p>pgloader support the following commands:</p>
<dl class='dl-vertical'>
  <dt>
    *
  </dt>
  <dd>
    <p><strong>LOAD CSV</strong></p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><strong>LOAD FIXED</strong></p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><strong>LOAD DBF</strong></p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><strong>LOAD SQLite</strong></p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><strong>LOAD MYSQL</strong></p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><strong>LOAD ARCHIVE</strong></p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><strong>LOAD DATABASE</strong></p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><strong>LOAD MESSAGES</strong></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>

</dl>
<p>The pgloader commands follow the same grammar rules. Each of them might support only a subset of the general options and provide specific options.</p><ul>
<li>
<pre>
LOAD &lt;something&gt;
     FROM &lt;source-url&gt;  [ WITH &lt;source-options&gt; ]
     INTO &lt;postgresql-url&gt;

[ WITH &lt;load-options&gt; ]

[ SET &lt;postgresql-settings&gt; ]
;
</pre>
</li><li></li>
</ul><p>The main clauses are the <strong>LOAD</strong>, <strong>FROM</strong>, <strong>INTO</strong> and <strong>WITH</strong> clauses that each command implements. Some command then implement the <strong>SET</strong> command, or some specific clauses such as the <strong>CAST</strong> clause.</p>
        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">COMMON CLAUSES</h2>
        <div class="sectioncontent">
<p>Some clauses are common to all commands:</p>
<dl class='dl-vertical'>
  <dt>
    *
  </dt>
  <dd>
    <p><em>FROM</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The <em>FROM</em> clause specifies where to read the data from, and each command introduces its own variant of sources. For instance, the <em>CSV</em> source supports <strong>inline</strong>, <strong>stdin</strong>, a filename, a quoted filename, and a <em>FILENAME MATCHING</em> clause (see above); whereas the <em>MySQL</em> source only supports a MySQL database URI specification.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>In all cases, the <em>FROM</em> clause is able to read its value from an environment variable when using the form <strong>GETENV \'varname\'</strong>.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>INTO</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The PostgreSQL connection URI must contains the name of the target table where to load the data into. That table must have already been created in PostgreSQL, and the name might be schema qualified.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The <em>INTO</em> target database connection URI can be parsed from the value of an environment variable when using the form <strong>GETENV \'varname\'</strong>.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Then <em>INTO</em> option also supports an optional comma separated list of target columns, which are either the name of an input <em>field</em> or the white space separated list of the target column name, its PostgreSQL data type and a <em>USING</em> expression.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The <em>USING</em> expression can be any valid Common Lisp form and will be read with the current package set to <strong>pgloader.transforms</strong>, so that you can use functions defined in that package, such as functions loaded dynamically with the <strong>--load</strong> command line parameter.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Each <em>USING</em> expression is compiled at runtime to native code.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>This feature allows pgloader to load any number of fields in a CSV file into a possibly different number of columns in the database, using custom code for that projection.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>WITH</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Set of options to apply to the command, using a global syntax of either:</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>key = value</em></p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>use option</em></p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>do not use option</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>See each specific command for details.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>SET</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>This clause allows to specify session parameters to be set for all the sessions opened by pgloader. It expects a list of parameter name, the equal sign, then the single-quoted value as a comma separated list.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The names and values of the parameters are not validated by pgloader, they are given as-is to PostgreSQL.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>BEFORE LOAD DO</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>You can run SQL queries against the database before loading the data from the <strong>CSV</strong> file. Most common SQL queries are <strong>CREATE TABLE IF NOT EXISTS</strong> so that the data can be loaded.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Each command must be <em>dollar-quoted</em>: it must begin and end with a double dollar sign, <strong>$$</strong>. Dollar-quoted queries are then comma separated. No extra punctuation is expected after the last SQL query.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>BEFORE LOAD EXECUTE</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Same behaviour as in the <em>BEFORE LOAD DO</em> clause. Allows you to read the SQL queries from a SQL file. Implements support for PostgreSQL dollar-quoting and the <strong>&#92;i</strong> and <strong>&#92;ir</strong> include facilities as in <strong>psql</strong> batch mode (where they are the same thing).</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>AFTER LOAD DO</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Same format as <em>BEFORE LOAD DO</em>, the dollar-quoted queries found in that section are executed once the load is done. That\'s the right time to create indexes and constraints, or re-enable triggers.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>AFTER LOAD EXECUTE</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Same behaviour as in the <em>AFTER LOAD DO</em> clause. Allows you to read the SQL queries from a SQL file. Implements support for PostgreSQL dollar-quoting and the <strong>&#92;i</strong> and <strong>&#92;ir</strong> include facilities as in <strong>psql</strong> batch mode (where they are the same thing).</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>

</dl>
<h3>Connection String</h3>
<p>The <strong>&lt;source-url&gt;</strong> parameter is expected to be given as a <em>Connection URI</em> as documented in the PostgreSQL documentation at http://www.postgresql.org/docs/9.3/static/libpq-connect.html#LIBPQ-CONNSTRING.</p><ul>
<li>
<pre>
postgresql://[user[:password]@][netloc][:port][/dbname][?schema.table]
</pre>
</li><li></li>
</ul><p>Where:</p>
<dl class='dl-vertical'>
  <dt>
    *
  </dt>
  <dd>
    <p><em>user</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Can contain any character, including colon (<strong>:</strong>) which must then be doubled (<strong>::</strong>) and at-sign (<strong>@</strong>) which must then be doubled (<strong>@@</strong>).</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When omitted, the <em>user</em> name defaults to the value of the <strong>PGUSER</strong> environment variable, and if it is unset, the value of the <strong>USER</strong> environment variable.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>password</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Can contain any character, including that at sign (<strong>@</strong>) which must then be doubled (<strong>@@</strong>). To leave the password empty, when the <em>user</em> name ends with at at sign, you then have to use the syntax user:@.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When omitted, the <em>password</em> defaults to the value of the <strong>PGPASSWORD</strong> environment variable if it is set, otherwise the password is left unset.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>netloc</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Can be either a hostname in dotted notation, or an ipv4, or an Unix domain socket path. Empty is the default network location, under a system providing <em>unix domain socket</em> that method is preferred, otherwise the <em>netloc</em> default to <strong>localhost</strong>.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>It\'s possible to force the <em>unix domain socket</em> path by using the syntax <strong>unix:/path/to/where/the/socket/file/is</strong>, so to force a non default socket path and a non default port, you would have:</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
<pre>
postgresql://unix:/tmp:54321/dbname
</pre>

  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The <em>netloc</em> defaults to the value of the <strong>PGHOST</strong> environment variable, and if it is unset, to either the default <strong>unix</strong> socket path when running on a Unix system, and <strong>localhost</strong> otherwise.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>dbname</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Should be a proper identifier (letter followed by a mix of letters, digits and the punctuation signs comma (<strong>,</strong>), dash (<strong>-</strong>) and underscore (<strong>_</strong>).</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When omitted, the <em>dbname</em> defaults to the value of the environment variable <strong>PGDATABASE</strong>, and if that is unset, to the <em>user</em> value as determined above.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>The only optional parameter should be a possibly qualified table name.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>

</dl>

<h3>Regular Expressions</h3>
<p>Several clauses listed in the following accept <em>regular expressions</em> with the following input rules:</p>
<dl class='dl-vertical'>
  <dt>
    *
  </dt>
  <dd>
    <p>A regular expression begins with a tilde sign (<strong>~</strong>),</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>is then followed with an opening sign,</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>then any character is allowed and considered part of the regular expression, except for the closing sign,</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>then a closing sign is expected.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>

</dl>
<p>The opening and closing sign are allowed by pair, here\'s the complete list of allowed delimiters:</p><ul>
<li>
<pre>
~//
~[]
~{}
~()
~&lt;&gt;
~""
~\'\'
~||
~##
</pre>
</li><li></li>
</ul><p>Pick the set of delimiters that don\'t collide with the <em>regular expression</em> you\'re trying to input. If your expression is such that none of the solutions allow you to enter it, the places where such expressions are allowed should allow for a list of expressions.</p>
<h3>Comments</h3>
<p>Any command may contain comments, following those input rules:</p>
<dl class='dl-vertical'>
  <dt>
    *
  </dt>
  <dd>
    <p>the <strong>--</strong> delimiter begins a comment that ends with the end of the current line,</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>the delimiters <strong>/*</strong> and <strong>*/</strong> respectively start and end a comment, which can be found in the middle of a command or span several lines.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>

</dl>
<p>Any place where you could enter a <em>whitespace</em> will accept a comment too.</p>
<h3>Batch behaviour options</h3>
<p>All pgloader commands have support for a <em>WITH</em> clause that allows for specifying options. Some options are generic and accepted by all commands, such as the <em>batch behaviour options</em>, and some options are specific to a data source kind, such as the CSV <em>skip header</em> option.</p><p>The global batch behaviour options are:</p>
<dl class='dl-vertical'>
  <dt>
    *
  </dt>
  <dd>
    <p><em>batch rows</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Takes a numeric value as argument, used as the maximum number of rows allowed in a batch. The default is <strong>25 000</strong> and can be changed to try having better performances characteristics or to control pgloader memory usage;</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>batch size</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Takes a memory unit as argument, such as <em>20 MB</em>, its default value. Accepted multipliers are <em>kB</em>, <em>MB</em>, <em>GB</em>, <em>TB</em> and <em>PB</em>. The case is important so as not to be confused about bits versus bytes, we\'re only talking bytes here.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>batch concurrency</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Takes a numeric value as argument, defaults to <strong>10</strong>. That\'s the number of batches that pgloader is allows to build in memory, even when only a single batch at a time might be sent to PostgreSQL.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Supporting more than a single batch being sent at a time is on the TODO list of pgloader, but is not implemented yet. This option is about controlling the memory needs of pgloader as a trade-off to the performances characteristics, and not about parallel activity of pgloader.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>

</dl>
<p>Other options are specific to each input source, please refer to specific parts of the documentation for their listing and covering.</p><p>A batch is then closed as soon as either the <em>batch rows</em> or the <em>batch size</em> threshold is crossed, whichever comes first. In cases when a batch has to be closed because of the <em>batch size</em> setting, a <em>debug</em> level log message is printed with how many rows did fit in the <em>oversized</em> batch.</p>

        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">LOAD CSV</h2>
        <div class="sectioncontent">
<p>This command instructs pgloader to load data from a <strong>CSV</strong> file. Here\'s an example:</p><ul>
<li>
<pre>
LOAD CSV
   FROM \'GeoLiteCity-Blocks.csv\' WITH ENCODING iso-646-us
        HAVING FIELDS
        (
           startIpNum, endIpNum, locId
        )
   INTO postgresql://user@localhost:54393/dbname?geolite.blocks
        TARGET COLUMNS
        (
           iprange ip4r using (ip-range startIpNum endIpNum),
           locId
        )
   WITH truncate,
        skip header = 2,
        fields optionally enclosed by \'"\',
        fields escaped by backslash-quote,
        fields terminated by \'&#92;t\'

    SET work_mem to \'32 MB\', maintenance_work_mem to \'64 MB\';
</pre>
</li><li></li>
</ul><p>The <strong>csv</strong> format command accepts the following clauses and options:</p>
<dl class='dl-vertical'>
  <dt>
    *
  </dt>
  <dd>
    <p><em>FROM</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Filename where to load the data from. Accepts an <em>ENCODING</em> option. Use the <strong>--list-encodings</strong> option to know which encoding names are supported.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The filename may be enclosed by single quotes, and could be one of the following special values:</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>inline</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The data is found after the end of the parsed commands. Any number of empty lines between the end of the commands and the beginning of the data is accepted.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>stdin</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Reads the data from the standard input stream.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>FILENAMES MATCHING</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The whole <em>matching</em> clause must follow the following rule:</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
<pre>
[ ALL FILENAMES | [ FIRST ] FILENAME ]
MATCHING regexp
[ IN DIRECTORY \'...\' ]
</pre>

  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The <em>matching</em> clause applies given <em>regular expression</em> (see above for exact syntax, several options can be used here) to filenames. It\'s then possible to load data from only the first match of all of them.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The optional <em>IN DIRECTORY</em> clause allows specifying which directory to walk for finding the data files, and can be either relative to where the command file is read from, or absolute. The given directory must exists.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The <em>FROM</em> option also supports an optional comma separated list of <em>field</em> names describing what is expected in the <strong>CSV</strong> data file, optionally introduced by the clause <strong>HAVING FIELDS</strong>.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Each field name can be either only one name or a name following with specific reader options for that field. Supported per-field reader options are:</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>terminated by</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>See the description of <em>field terminated by</em> below.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The processing of this option is not currently implemented.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>date format</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When the field is expected of the date type, then this option allows to specify the date format used in the file.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The processing of this option is not currently implemented.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>null if</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>This option takes an argument which is either the keyword <em>blanks</em> or a double-quoted string.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When <em>blanks</em> is used and the field value that is read contains only space characters, then it\'s automatically converted to an SQL <strong>NULL</strong> value.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When a double-quoted string is used and that string is read as the field value, then the field value is automatically converted to an SQL <strong>NULL</strong> value.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>WITH</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When loading from a <strong>CSV</strong> file, the following options are supported:</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>truncate</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader issues a <strong>TRUNCATE</strong> command against the PostgreSQL target table before reading the data file.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>skip header</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Takes a numeric value as argument. Instruct pgloader to skip that many lines at the beginning of the input file.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>trim unquoted blanks</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When reading unquoted values in the <strong>CSV</strong> file, remove the blanks found in between the separator and the value. That behaviour is the default.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>keep unquoted blanks</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When reading unquoted values in the <strong>CSV</strong> file, keep blanks found in between the separator and the value.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>fields optionally enclosed by</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Takes a single character as argument, which must be found inside single quotes, and might be given as the printable character itself, the special value &#92;t to denote a tabulation character, or <strong>0x</strong> then an hexadecimal value read as the ASCII code for the character.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>This character is used as the quoting character in the <strong>CSV</strong> file, and defaults to double-quote.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>fields not enclosed</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>By default, pgloader will use the double-quote character as the enclosing character. If you have a CSV file where fields are not enclosed and are using double-quote as an expected ordinary character, then use the option <em>fields not enclosed</em> for the CSV parser to accept those values.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>fields escaped by</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Takes either the special value <em>backslash-quote</em> or <em>double-quote</em>. This value is used to recognize escaped field separators when they are to be found within the data fields themselves. Defaults to <em>double-quote</em>.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>fields terminated by</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Takes a single character as argument, which must be found inside single quotes, and might be given as the printable character itself, the special value &#92;t to denote a tabulation character, or <strong>0x</strong> then an hexadecimal value read as the ASCII code for the character.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>This character is used as the <em>field separator</em> when reading the <strong>CSV</strong> data.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>lines terminated by</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Takes a single character as argument, which must be found inside single quotes, and might be given as the printable character itself, the special value &#92;t to denote a tabulation character, or <strong>0x</strong> then an hexadecimal value read as the ASCII code for the character.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>This character is used to recognize <em>end-of-line</em> condition when reading the <strong>CSV</strong> data.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>

</dl>

        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">LOAD FIXED COLS</h2>
        <div class="sectioncontent">
<p>This command instructs pgloader to load data from a text file containing columns arranged in a <em>fixed size</em> manner. Here\'s an example:</p><ul>
<li>
<pre>
LOAD FIXED
     FROM inline (a 0 10, b 10 8, c 18 8, d 26 17)
     INTO postgresql:///pgloader?fixed
          (
             a, b,
             c time using (time-with-no-separator c),
             d
          )

     WITH truncate

      SET client_encoding to \'latin1\',
          work_mem to \'14MB\',
          standard_conforming_strings to \'on\'

BEFORE LOAD DO
     $$ drop table if exists fixed; $$,
     $$ create table fixed (
         a integer,
         b date,
         c time,
         d text
        );
     $$;

 01234567892008052011431250firstline
    01234562008052115182300left blank-padded
 12345678902008052208231560another line
</pre>
</li><li></li>
</ul><p>The <strong>fixed</strong> format command accepts the following clauses and options:</p>
<dl class='dl-vertical'>
  <dt>
    *
  </dt>
  <dd>
    <p><em>FROM</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Filename where to load the data from. Accepts an <em>ENCODING</em> option. Use the <strong>--list-encodings</strong> option to know which encoding names are supported.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The filename may be enclosed by single quotes, and could be one of the following special values:</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>inline</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The data is found after the end of the parsed commands. Any number of empty lines between the end of the commands and the beginning of the data is accepted.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>stdin</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Reads the data from the standard input stream.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The <em>FROM</em> option also supports an optional comma separated list of <em>field</em> names describing what is expected in the <strong>FIXED</strong> data file.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Each field name is composed of the field name followed with specific reader options for that field. Supported per-field reader options are the following, where only <em>start</em> and <em>length</em> are required.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>start</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Position in the line where to start reading that field\'s value. Can be entered with decimal digits or <strong>0x</strong> then hexadecimal digits.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>length</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>How many bytes to read from the <em>start</em> position to read that field\'s value. Same format as <em>start</em>.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>terminated by</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>See the description of <em>field terminated by</em> below.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The processing of this option is not currently implemented.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>date format</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When the field is expected of the date type, then this option allows to specify the date format used in the file.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The processing of this option is not currently implemented.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>null if</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>This option takes an argument which is either the keyword <em>blanks</em> or a double-quoted string.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When <em>blanks</em> is used and the field value that is read contains only space characters, then it\'s automatically converted to an SQL <strong>NULL</strong> value.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When a double-quoted string is used and that string is read as the field value, then the field value is automatically converted to an SQL <strong>NULL</strong> value.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>WITH</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When loading from a <strong>CSV</strong> file, the following options are supported:</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>truncate</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader issues a <strong>TRUNCATE</strong> command against the PostgreSQL target table before reading the data file.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>skip header</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Takes a numeric value as argument. Instruct pgloader to skip that many lines at the beginning of the input file.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>

</dl>

        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">LOAD DBF</h2>
        <div class="sectioncontent">
<p>This command instructs pgloader to load data from a <strong>DBF</strong> file. Here\'s an example:</p><ul>
<li>
<pre>
LOAD DBF
    FROM http://www.insee.fr/fr/methodes/nomenclatures/cog/telechargement/2013/dbf/reg2013.dbf
    INTO postgresql://user@localhost/dbname
    WITH truncate, create table;
</pre>
</li><li></li>
</ul><p>The <strong>dbf</strong> format command accepts the following clauses and options:</p>
<dl class='dl-vertical'>
  <dt>
    *
  </dt>
  <dd>
    <p><em>FROM</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Filename where to load the data from. This support local files, HTTP URLs and zip files containing a single dbf file of the same name. Fetch such a zip file from an HTTP address is of course supported.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>WITH</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When loading from a <strong>DBF</strong> file, the following options are supported:</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>truncate</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader issues a <strong>TRUNCATE</strong> command against the PostgreSQL target table before reading the data file.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>create table</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader creates the table using the meta data found in the <strong>DBF</strong> file, which must contain a list of fields with their data type. A standard data type conversion from DBF to PostgreSQL is done.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>table name</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>This options expects as its value the possibly qualified name of the table to create.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>

</dl>

        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">LOAD IXF</h2>
        <div class="sectioncontent">
<p>This command instructs pgloader to load data from an IBM <strong>IXF</strong> file. Here\'s an example:</p><ul>
<li>
<pre>
LOAD IXF
    FROM data/nsitra.test1.ixf
    INTO postgresql:///pgloader?nsitra.test1
    WITH truncate, create table

  BEFORE LOAD DO
   $$ create schema if not exists nsitra; $$,
   $$ drop table if exists nsitra.test1; $$;
</pre>
</li><li></li>
</ul><p>The <strong>ixf</strong> format command accepts the following clauses and options:</p>
<dl class='dl-vertical'>
  <dt>
    *
  </dt>
  <dd>
    <p><em>FROM</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Filename where to load the data from. This support local files, HTTP URLs and zip files containing a single ixf file of the same name. Fetch such a zip file from an HTTP address is of course supported.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>WITH</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When loading from a <strong>IXF</strong> file, the following options are supported:</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>truncate</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader issues a <strong>TRUNCATE</strong> command against the PostgreSQL target table before reading the data file.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>create table</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader creates the table using the meta data found in the <strong>DBF</strong> file, which must contain a list of fields with their data type. A standard data type conversion from DBF to PostgreSQL is done.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>table name</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>This options expects as its value the possibly qualified name of the table to create.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>

</dl>

        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">LOAD ARCHIVE</h2>
        <div class="sectioncontent">
<p>This command instructs pgloader to load data from one or more files contained in an archive. Currently the only supported archive format is <em>ZIP</em>, and the archive might be downloaded from an <em>HTTP</em> URL.</p><p>Here\'s an example:</p><ul>
<li>
<pre>
LOAD ARCHIVE
   FROM /Users/dim/Downloads/GeoLiteCity-latest.zip
   INTO postgresql:///ip4r

   BEFORE LOAD DO
     $$ create extension if not exists ip4r; $$,
     $$ create schema if not exists geolite; $$,
     $$ create table if not exists geolite.location
       (
          locid      integer primary key,
          country    text,
          region     text,
          city       text,
          postalcode text,
          location   point,
          metrocode  text,
          areacode   text
       );
     $$,
     $$ create table if not exists geolite.blocks
       (
          iprange    ip4r,
          locid      integer
       );
     $$,
     $$ drop index if exists geolite.blocks_ip4r_idx; $$,
     $$ truncate table geolite.blocks, geolite.location cascade; $$

   LOAD CSV
        FROM FILENAME MATCHING ~/GeoLiteCity-Location.csv/
             WITH ENCODING iso-8859-1
             (
                locId,
                country,
                region     null if blanks,
                city       null if blanks,
                postalCode null if blanks,
                latitude,
                longitude,
                metroCode  null if blanks,
                areaCode   null if blanks
             )
        INTO postgresql:///ip4r?geolite.location
             (
                locid,country,region,city,postalCode,
                location point using (format nil "(~a,~a)" longitude latitude),
                metroCode,areaCode
             )
        WITH skip header = 2,
             fields optionally enclosed by \'"\',
             fields escaped by double-quote,
             fields terminated by \',\'

  AND LOAD CSV
        FROM FILENAME MATCHING ~/GeoLiteCity-Blocks.csv/
             WITH ENCODING iso-8859-1
             (
                startIpNum, endIpNum, locId
             )
        INTO postgresql:///ip4r?geolite.blocks
             (
                iprange ip4r using (ip-range startIpNum endIpNum),
                locId
             )
        WITH skip header = 2,
             fields optionally enclosed by \'"\',
             fields escaped by double-quote,
             fields terminated by \',\'

   FINALLY DO
     $$ create index blocks_ip4r_idx on geolite.blocks using gist(iprange); $$;
</pre>
</li><li></li>
</ul><p>The <strong>archive</strong> command accepts the following clauses and options:</p>
<dl class='dl-vertical'>
  <dt>
    *
  </dt>
  <dd>
    <p><em>FROM</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Filename or HTTP URI where to load the data from. When given an HTTP URL the linked file will get downloaded locally before processing.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>If the file is a <strong>zip</strong> file, the command line utility <strong>unzip</strong> is used to expand the archive into files in <strong>$TMPDIR</strong>, or <strong>/tmp</strong> if <strong>$TMPDIR</strong> is unset or set to a non-existing directory.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Then the following commands are used from the top level directory where the archive has been expanded.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>command [ <em>AND</em> command ... ]</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>A series of commands against the contents of the archive, at the moment only <strong>CSV</strong>,<strong>\'FIXED</strong> and <strong>DBF</strong> commands are supported.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Note that commands are supporting the clause <em>FROM FILENAME MATCHING</em> which allows the pgloader command not to depend on the exact names of the archive directories.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The same clause can also be applied to several files with using the spelling <em>FROM ALL FILENAMES MATCHING</em> and a regular expression.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The whole <em>matching</em> clause must follow the following rule:</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
<pre>
 FROM [ ALL FILENAMES | [ FIRST ] FILENAME ] MATCHING
</pre>

  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>FINALLY DO</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>SQL Queries to run once the data is loaded, such as <strong>CREATE INDEX</strong>.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>

</dl>

        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">LOAD MYSQL DATABASE</h2>
        <div class="sectioncontent">
<p>This command instructs pgloader to load data from a database connection. The only supported database source is currently <em>MySQL</em>, and pgloader supports dynamically converting the schema of the source database and the indexes building.</p><p>A default set of casting rules are provided and might be overloaded and appended to by the command.</p><p>Here\'s an example:</p><ul>
<li>
<pre>
LOAD DATABASE
     FROM      mysql://root@localhost/sakila
     INTO postgresql://localhost:54393/sakila

 WITH include drop, create tables, create indexes, reset sequences

  SET maintenance_work_mem to \'128MB\',
      work_mem to \'12MB\',
      search_path to \'sakila\'

 CAST type datetime to timestamptz drop default drop not null using zero-dates-to-null,
      type date drop not null drop default using zero-dates-to-null,
      -- type tinyint to boolean using tinyint-to-boolean,
      type year to integer

 MATERIALIZE VIEWS film_list, staff_list

 -- INCLUDING ONLY TABLE NAMES MATCHING ~/film/, \'actor\'
 -- EXCLUDING TABLE NAMES MATCHING ~&lt;ory&gt;
 -- DECODING TABLE NAMES MATCHING ~/messed/, ~/encoding/ AS utf8

 BEFORE LOAD DO
 $$ create schema if not exists sakila; $$;
</pre>
</li><li></li>
</ul><p>The <strong>database</strong> command accepts the following clauses and options:</p>
<dl class='dl-vertical'>
  <dt>
    *
  </dt>
  <dd>
    <p><em>FROM</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Must be a connection URL pointing to a MySQL database. At the moment only MySQL is supported as a pgloader source.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>If the connection URI contains a table name, then only this table is migrated from MySQL to PostgreSQL.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>WITH</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When loading from a <strong>MySQL</strong> database, the following options are supported:</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>include drop</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader drop in the PostgreSQL connection all the table whose names have been found in the MySQL database. This option allows for using the same command several times in a row until you figure out all the options, starting automatically from a clean environment.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>include no drop</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader will not include any <strong>DROP</strong> statement when loading the data.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>truncate</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader issue the <strong>TRUNCATE</strong> command against each PostgreSQL table just before loading data into it.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>no truncate</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader issues no <strong>TRUNCATE</strong> command.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>create tables</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader creates the table using the meta data found in the <strong>MySQL</strong> file, which must contain a list of fields with their data type. A standard data type conversion from DBF to PostgreSQL is done.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>create no tables</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader skips the creation of table before lading data, target tables must then already exist.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>create indexes</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader gets the definitions of all the indexes found in the MySQL database and create the same set of index definitions against the PostgreSQL database.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>create no indexes</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader skips the creating indexes.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>foreign keys</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader gets the definitions of all the foreign keys found in the MySQL database and create the same set of foreign key definitions against the PostgreSQL database.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>no foreign keys</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader skips creating foreign keys.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>reset sequences</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, at the end of the data loading and after the indexes have all been created, pgloader resets all the PostgreSQL sequences created to the current maximum value of the column they are attached to.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The options <em>schema only</em> and <em>data only</em> have no effects on this option.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>reset no sequences</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader skips resetting sequences after the load.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The options <em>schema only</em> and <em>data only</em> have no effects on this option.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>downcase identifiers</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader converts all MySQL identifiers (table names, index names, column names) to <em>downcase</em>, except for PostgreSQL <em>reserved</em> keywords.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The PostgreSQL <em>reserved</em> keywords are determined dynamically by using the system function <strong>pg_get_keywords()</strong>.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>quote identifiers</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader quotes all MySQL identifiers so that their case is respected. Note that you will then have to do the same thing in your application code queries.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>schema only</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed pgloader refrains from migrating the data over. Note that the schema in this context includes the indexes when the option <em>create indexes</em> has been listed.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>data only</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed pgloader only issues the <strong>COPY</strong> statements, without doing any other processing.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>CAST</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The cast clause allows to specify custom casting rules, either to overload the default casting rules or to amend them with special cases.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>A casting rule is expected to follow one of the forms:</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
<pre>
type &lt;mysql-type-name&gt; [ &lt;guard&gt; ... ] to &lt;pgsql-type-name&gt; [ &lt;option&gt; ... ]
column &lt;table-name&gt;.&lt;column-name&gt; [ &lt;guards&gt; ] to ...
</pre>

  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>It\'s possible for a <em>casting rule</em> to either match against a MySQL data type or against a given <em>column name</em> in a given <em>table name</em>. That flexibility allows to cope with cases where the type <strong>tinyint</strong> might have been used as a <strong>boolean</strong> in some cases but as a <strong>smallint</strong> in others.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The <em>casting rules</em> are applied in order, the first match prevents following rules to be applied, and user defined rules are evaluated first.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The supported guards are:</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>when default \'value\'</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The casting rule is only applied against MySQL columns of the source type that have given <em>value</em>, which must be a single-quoted or a double-quoted string.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>when typemod expression</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The casting rule is only applied against MySQL columns of the source type that have a <em>typemod</em> value matching the given <em>typemod expression</em>. The <em>typemod</em> is separated into its <em>precision</em> and <em>scale</em> components.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Example of a cast rule using a <em>typemod</em> guard:</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
<pre>
type char when (= precision 1) to char keep typemod
</pre>

  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>This expression casts MySQL <strong>char</strong>(1)</strong> column to a PostgreSQL column of type <strong>char</strong>(1)</strong> while allowing for the general case <strong>char(N)</strong> will be converted by the default cast rule into a PostgreSQL type <strong>varchar(N)</strong>.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The supported casting options are:</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>drop default</em>, <em>keep default</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When the option <em>drop default</em> is listed, pgloader drops any existing default expression in the MySQL database for columns of the source type from the <strong>CREATE TABLE</strong> statement it generates.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The spelling <em>keep default</em> explicitly prevents that behaviour and can be used to overload the default casting rules.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>drop not null</em>, <em>keep not null</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When the option <em>drop not null</em> is listed, pgloader drops any existing <strong>NOT NULL</strong> constraint associated with the given source MySQL datatype when it creates the tables in the PostgreSQL database.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The spelling <em>keep not null</em> explicitly prevents that behaviour and can be used to overload the default casting rules.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>drop typemod</em>, <em>keep typemod</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When the option <em>drop typemod</em> is listed, pgloader drops any existing <em>typemod</em> definition (e.g. <em>precision</em> and <em>scale</em>) from the datatype definition found in the MySQL columns of the source type when it created the tables in the PostgreSQL database.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The spelling <em>keep typemod</em> explicitly prevents that behaviour and can be used to overload the default casting rules.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>using</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>This option takes as its single argument the name of a function to be found in the <strong>pgloader.transforms</strong> Common Lisp package. See above for details.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>It\'s possible to augment a default cast rule (such as one that applies against <strong>ENUM</strong> data type for example) with a <em>transformation function</em> by omitting entirely the <strong>type</strong> parts of the casting rule, as in the following example:</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
<pre>
column enumerate.foo using empty-string-to-null
</pre>

  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>MATERIALIZE VIEWS</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>This clause allows you to implement custom data processing at the data source by providing a <em>view definition</em> against which pgloader will query the data. It\'s not possible to just allow for plain <strong>SQL</strong> because we want to know a lot about the exact data types of each column involved in the query output.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>This clause expect a comma separated list of view definitions, each one being either the name of an existing view in your database or the following expression:</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p><em>name</em> <strong>AS</strong> <strong>$$</strong> <em>sql query</em> <strong>$$</strong></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The <em>name</em> and the <em>sql query</em> will be used in a <strong>CREATE VIEW</strong> statement at the beginning of the data loading, and the resulting view will then be dropped at the end of the data loading.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>MATERIALIZE ALL VIEWS</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Same behaviour as <em>MATERIALIZE VIEWS</em> using the dynamic list of views as returned by MySQL rather than asking the user to specify the list.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>INCLUDING ONLY TABLE NAMES MATCHING</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Introduce a comma separated list of table names or <em>regular expression</em> used to limit the tables to migrate to a sublist.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Example:</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
<pre>
INCLUDING ONLY TABLE NAMES MATCHING ~/film/, \'actor\'
</pre>

  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>EXCLUDING TABLE NAMES MATCHING</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Introduce a comma separated list of table names or <em>regular expression</em> used to exclude table names from the migration. This filter only applies to the result of the <em>INCLUDING</em> filter.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
<pre>
EXCLUDING TABLE NAMES MATCHING ~&lt;ory&gt;
</pre>

  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>DECODING TABLE NAMES MATCHING</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Introduce a comma separated list of table names or <em>regular expressions</em> used to force the encoding to use when processing data from MySQL. If the data encoding known to you is different from MySQL\'s idea about it, this is the option to use.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
<pre>
DECODING TABLE NAMES MATCHING ~/messed/, ~/encoding/ AS utf8
</pre>

  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>You can use as many such rules as you need, all with possibly different encodings.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>

</dl>
<h3>LIMITATIONS</h3>
<p>The <strong>database</strong> command currently only supports MySQL source database and has the following limitations:</p>
<dl class='dl-vertical'>
  <dt>
    *
  </dt>
  <dd>
    <p>Views are not migrated,</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Supporting views might require implementing a full SQL parser for the MySQL dialect with a porting engine to rewrite the SQL against PostgreSQL, including renaming functions and changing some constructs.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>While it\'s not theoretically impossible, don\'t hold your breath.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>Triggers are not migrated</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The difficulty of doing so is not yet assessed.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><strong>ON UPDATE CURRENT_TIMESTAMP</strong> is currently not migrated</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>It\'s simple enough to implement, just not on the priority list yet.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>Of the geometric datatypes, only the <strong>POINT</strong> database has been covered. The other ones should be easy enough to implement now, it\'s just not done yet.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>

</dl>

<h3>DEFAULT MySQL CASTING RULES</h3>
<p>When migrating from MySQL the following Casting Rules are provided:</p><p>Numbers:</p>
<dl class='dl-vertical'>
  <dt>
    *
  </dt>
  <dd>
    <p>type int with extra auto_increment to serial when (&lt; precision 10)</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type int with extra auto_increment to bigserial when (&lt;= 10 precision)</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type int to int when (&lt; precision 10)</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type int to bigint when (&lt;= 10 precision)</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type smallint with extra auto_increment to serial</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type bigint with extra auto_increment to bigserial</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type tinyint to boolean when (= 1 precision) using tinyint-to-boolean</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type tinyint to smallint drop typemod</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type smallint to smallint drop typemod</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type mediumint to integer drop typemod</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type integer to integer drop typemod</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type float to float drop typemod</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type bigint to bigint drop typemod</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type double to double precision drop typemod</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type numeric to numeric keep typemod</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type decimal to decimal keep typemod</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>

</dl>
<p>Texts:</p>
<dl class='dl-vertical'>
  <dt>
    *
  </dt>
  <dd>
    <p>type char to varchar keep typemod</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type varchar to text</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type tinytext to text</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type text to text</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type mediumtext to text</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type longtext to text</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>

</dl>
<p>Binary:</p>
<dl class='dl-vertical'>
  <dt>
    *
  </dt>
  <dd>
    <p>type binary to bytea</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type varbinary to bytea</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type tinyblob to bytea</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type blob to bytea</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type mediumblob to bytea</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type longblob to bytea</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>

</dl>
<p>Date:</p>
<dl class='dl-vertical'>
  <dt>
    *
  </dt>
  <dd>
    <p>type datetime when default "0000-00-00 00:00:00" and not null to timestamptz drop not null drop default using zero-dates-to-null</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type datetime when default "0000-00-00 00:00:00" to timestamptz drop default using zero-dates-to-null</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type timestamp when default "0000-00-00 00:00:00" and not null to timestamptz drop not null drop default using zero-dates-to-null</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type timestamp when default "0000-00-00 00:00:00" to timestamptz drop default using zero-dates-to-null</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type date when default "0000-00-00" to date drop default using zero-dates-to-null</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type date to date</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type datetime to timestamptz</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type timestamp to timestamptz</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p>type year to integer drop typemod</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>

</dl>
<p>Geometric:</p>
<dl class='dl-vertical'>
  <dt>
    *
  </dt>
  <dd>
    <p>type point to point using pgloader.transforms::convert-mysql-point</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>

</dl>
<p>Enum types are declared inline in MySQL and separately with a <strong>CREATE TYPE</strong> command in PostgreSQL, so each column of Enum Type is converted to a type named after the table and column names defined with the same labels in the same order.</p><p>When the source type definition is not matched in the default casting rules nor in the casting rules provided in the command, then the type name with the typemod is used.</p>

        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">LOAD SQLite DATABASE</h2>
        <div class="sectioncontent">
<p>This command instructs pgloader to load data from a SQLite file. Automatic discovery of the schema is supported, including build of the indexes.</p><p>Here\'s an example:</p><ul>
<li>
<pre>
load database
     from sqlite:///Users/dim/Downloads/lastfm_tags.db
     into postgresql:///tags

 with include drop, create tables, create indexes, reset sequences

  set work_mem to \'16MB\', maintenance_work_mem to \'512 MB\';
</pre>
</li><li></li>
</ul><p>The <strong>sqlite</strong> command accepts the following clauses and options:</p>
<dl class='dl-vertical'>
  <dt>
    *
  </dt>
  <dd>
    <p><em>FROM</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Path or HTTP URL to a SQLite file, might be a <strong>.zip</strong> file.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>WITH</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When loading from a <strong>SQLite</strong> database, the following options are supported:</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>include drop</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader drop in the PostgreSQL connection all the table whose names have been found in the SQLite database. This option allows for using the same command several times in a row until you figure out all the options, starting automatically from a clean environment.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>include no drop</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader will not include any <strong>DROP</strong> statement when loading the data.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>truncate</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader issue the <strong>TRUNCATE</strong> command against each PostgreSQL table just before loading data into it.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>no truncate</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader issues no <strong>TRUNCATE</strong> command.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>create tables</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader creates the table using the meta data found in the <strong>SQLite</strong> file, which must contain a list of fields with their data type. A standard data type conversion from DBF to PostgreSQL is done.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>create no tables</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader skips the creation of table before lading data, target tables must then already exist.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>create indexes</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader gets the definitions of all the indexes found in the SQLite database and create the same set of index definitions against the PostgreSQL database.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>create no indexes</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader skips the creating indexes.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>reset sequences</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, at the end of the data loading and after the indexes have all been created, pgloader resets all the PostgreSQL sequences created to the current maximum value of the column they are attached to.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>reset no sequences</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed, pgloader skips resetting sequences after the load.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>The options <em>schema only</em> and <em>data only</em> have no effects on this option.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>schema only</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed pgloader will refrain from migrating the data over. Note that the schema in this context includes the indexes when the option <em>create indexes</em> has been listed.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>data only</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When this option is listed pgloader only issues the <strong>COPY</strong> statements, without doing any other processing.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>encoding</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>This option allows to control which encoding to parse the SQLite text data with. Defaults to UTF-8.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>INCLUDING ONLY TABLE NAMES MATCHING</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Introduce a comma separated list of table names or <em>regular expression</em> used to limit the tables to migrate to a sublist.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Example:</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
<pre>
INCLUDING ONLY TABLE NAMES MATCHING ~/film/, \'actor\'
</pre>

  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>EXCLUDING TABLE NAMES MATCHING</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Introduce a comma separated list of table names or <em>regular expression</em> used to exclude table names from the migration. This filter only applies to the result of the <em>INCLUDING</em> filter.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
<pre>
EXCLUDING TABLE NAMES MATCHING ~&lt;ory&gt;
</pre>

  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>

</dl>

        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">TRANSFORMATION FUNCTIONS</h2>
        <div class="sectioncontent">
<p>Some data types are implemented in a different enough way that a transformation function is necessary. This function must be written in <strong>Common lisp</strong> and is searched in the <strong>pgloader.transforms</strong> package.</p><p>Some default transformation function are provided with pgloader, and you can use the <strong>--load</strong> command line option to load and compile your own lisp file into pgloader at runtime. For your functions to be found, remember to begin your lisp file with the following form:</p><ul>
<li>
<pre>
(in-package #:pgloader.transforms)
</pre>
</li><li></li>
</ul><p>The provided transformation functions are:</p>
<dl class='dl-vertical'>
  <dt>
    *
  </dt>
  <dd>
    <p><em>zero-dates-to-null</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>When the input date is all zeroes, return <strong>nil</strong>, which gets loaded as a PostgreSQL <strong>NULL</strong> value.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>date-with-no-separator</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Applies <em>zero-dates-to-null</em> then transform the given date into a format that PostgreSQL will actually process:</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
<pre>
In:  "20041002152952"
Out: "2004-10-02 15:29:52"
</pre>

  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>tinyint-to-boolean</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>As MySQL lacks a proper boolean type, <em>tinyint</em> is often used to implement that. This function transforms <strong>0</strong> to <strong>\'false\'</strong> and anything else to <strong>\'true</strong>\'.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>int-to-ip</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Convert an integer into a dotted representation of an ip4.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
<pre>
In:  18435761
Out: "1.25.78.177"
</pre>

  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>ip-range</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Converts a couple of integers given as strings into a range of ip4.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
<pre>
In:  "16825344" "16825599"
Out: "1.0.188.0-1.0.188.255"
</pre>

  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>convert-mysql-point</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Converts from the <strong>astext</strong> representation of points in MySQL to the PostgreSQL representation.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
<pre>
In:  "POINT(48.5513589 7.6926827)"
Out: "(48.5513589,7.6926827)"
</pre>

  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>float-to-string</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Converts a Common Lisp float into a string suitable for a PostgreSQL float:</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
<pre>
In:  100.0d0
Out: "100.0"
</pre>

  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>set-to-enum-array</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Converts a string representing a MySQL SET into a PostgreSQL Array of Enum values from the set.</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
<pre>
In: "foo,bar"
Out: "{foo,bar}"
</pre>

  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>right-trimg</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Remove whitespace at end of string.</p>
  </dd>
  <dt>
    *
  </dt>
  <dd>
    <p><em>byte-vector-to-bytea</em></p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    <p>Transform a simple array of unsigned bytes to the PostgreSQL bytea Hex Format representation as documented at http://www.postgresql.org/docs/9.3/interactive/datatype-binary.html</p>
  </dd>
  <dt>
    
  </dt>
  <dd>
    
  </dd>

</dl>

        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">LOAD MESSAGES</h2>
        <div class="sectioncontent">
<p>This command is still experimental and allows receiving messages via UDP using a syslog like format, and, depending on rule matching, loads named portions of the data stream into a destination table.</p><ul>
<li>
<pre>
LOAD MESSAGES
    FROM syslog://localhost:10514/

 WHEN MATCHES rsyslog-msg IN apache
  REGISTERING timestamp, ip, rest
         INTO postgresql://localhost/db?logs.apache
          SET guc_1 = \'value\', guc_2 = \'other value\'

 WHEN MATCHES rsyslog-msg IN others
  REGISTERING timestamp, app-name, data
         INTO postgresql://localhost/db?logs.others
          SET guc_1 = \'value\', guc_2 = \'other value\'

    WITH apache = rsyslog
         DATA   = IP REST
         IP     = 1*3DIGIT "." 1*3DIGIT "."1*3DIGIT "."1*3DIGIT
         REST   = ~/.*/

    WITH others = rsyslog;
</pre>
</li><li></li>
</ul><p>As the command is still experimental the options might be changed in the future and the details are not documented.</p>
        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">AUTHOR</h2>
        <div class="sectioncontent">
<p>Dimitri Fontaine <em>dimitri@2ndQuadrant.fr</em></p>
        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">RELATED TO pgloader&hellip;</h2>
        <div class="sectioncontent">
<p>PostgreSQL COPY documentation at <em>http://www.postgresql.org/docs/9.3/static/sql-copy.html</em>.</p><p>The pgloader source code and all documentation may be downloaded from <em>http://tapoueh.org/pgloader/</em>.</p>
        </div>
      </section>
<nav>
  <ul class="pager">
   <li class="previous"><a href="pgfouine_vacuum.1.html"><span aria-hidden="true">&larr;</span> pgfouine_vacuum.1: Postgresql vacuum log analyzer</a></li>
   <li class="next"><a href="pgmbentley.1.html">pgmbentley.1: Bentleyize a portable graymap <span aria-hidden="true">&rarr;</span></a></li>
  </ul>
</nav>

  </div>
  <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
  <!-- Include all compiled plugins (below), or include individual files as needed -->
  <script src="/js/bootstrap.min.js"></script>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-60781387-1', 'auto');
    ga('send', 'pageview');

  </script>
</body>
</html>
