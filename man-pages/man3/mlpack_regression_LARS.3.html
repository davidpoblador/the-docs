<!DOCTYPE html>
<html lang="en">
<head>
  <link href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700&effect=destruction%7Cshadow-multiple" rel="stylesheet" type="text/css">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>mlpack_regression_LARS: An implementation of lars, a stage-wise homotopy-based algorithm for l1-regularized linear regression (lasso) and l1+l2 regularized linear regression (elastic net).</title>
  <link href="/css/bootstrap.min.css" rel="stylesheet">
  <link href="/css/manpage.css" rel="stylesheet">
  <link rel="apple-touch-icon" sizes="57x57" href="/icons/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/icons/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/icons/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/icons/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/icons/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/icons/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/icons/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/icons/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192" href="/icons/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/icons/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png">
  <link rel="manifest" href="/icons/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/icons/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">
  <meta name="description" content="An implementation of lars, a stage-wise homotopy-based algorithm for l1-regularized linear regression (lasso) and l1+l2 regularized linear regression (elastic net).">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@CartaTech">
  <meta name="twitter:creator" content="@CartaTech">
  <meta name="twitter:title" content="mlpack_regression_LARS (3) manual">
  <meta name="twitter:description" content="An implementation of lars, a stage-wise homotopy-based algorithm for l1-regularized linear regression (lasso) and l1+l2 regularized linear regression (elastic net).">
  <meta name="twitter:image" content="https://www.carta.tech/images/mlpack-doc-mlpack_regression_LARS-3.png">
  <meta property="og:url" content="https://www.carta.tech/man-pages/man3/mlpack_regression_LARS.3.html" />
  <meta property="og:type" content="website" />
  <meta property="og:title" content="mlpack_regression_LARS (3) manual" />
  <meta property="og:description" content="An implementation of lars, a stage-wise homotopy-based algorithm for l1-regularized linear regression (lasso) and l1+l2 regularized linear regression (elastic net)." />
  <meta property="fb:app_id" content="1241677679199500" />
  <meta property="og:image" content="https://www.carta.tech/images/mlpack-doc-mlpack_regression_LARS-3.png" />
  <meta property="og:image:width" content="600" />
  <meta property="og:image:height" content="315" />
</head>
<body>
  <div class="container final">
          <div class="page-header">
        <h1 class="font-effect-destruction">mlpack_regression_LARS<small> (3)</small></h1>
        <p class="lead">An implementation of lars, a stage-wise homotopy-based algorithm for l1-regularized linear regression (lasso) and l1+l2 regularized linear regression (elastic net).</p>
      </div>

    <ol class="breadcrumb" itemscope itemtype="http://schema.org/BreadcrumbList">
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/">
      <span itemprop="name">Carta.tech</span>
    </a>
    <meta itemprop="position" content="1" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/man-pages/">
      <span itemprop="name">Man Pages</span>
    </a>
    <meta itemprop="position" content="2" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/man-pages/man3/">
      <span itemprop="name">Library calls</span>
    </a>
    <meta itemprop="position" content="3" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/man-pages/man3/mlpack_regression_LARS.3.html">
      <span itemprop="name">mlpack_regression_LARS: An implementation of lars, a stage-wise homotopy-based algorithm for l1-regularized linear regression (lasso) and l1+l2 regularized linear regression (elastic net).</span>
    </a>
    <meta itemprop="position" content="4" />
  </li>
</ol>
<ol class="breadcrumb" itemscope itemtype="http://schema.org/BreadcrumbList">
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/">
      <span itemprop="name">Carta.tech</span>
    </a>
    <meta itemprop="position" content="1" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/packages/">
      <span itemprop="name">Packages</span>
    </a>
    <meta itemprop="position" content="2" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/packages/mlpack-doc/">
      <span itemprop="name">mlpack-doc</span>
    </a>
    <meta itemprop="position" content="3" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/man-pages/man3/mlpack_regression_LARS.3.html">
      <span itemprop="name">mlpack_regression_LARS: An implementation of lars, a stage-wise homotopy-based algorithm for l1-regularized linear regression (lasso) and l1+l2 regularized linear regression (elastic net).</span>
    </a>
    <meta itemprop="position" content="4" />
  </li>
</ol>
    
      <section>
        <h2 class="font-effect-shadow-multiple">SYNOPSIS</h2>
        <div class="sectioncontent">
<h3>Public Member Functions</h3>
<p><strong>LARS</strong> (const bool <strong>useCholesky</strong>, const double <strong>lambda1</strong>=0.0, const double <strong>lambda2</strong>=0.0, const double <strong>tolerance</strong>=1e-16)</p><p><em>Set the parameters to </em><strong>LARS</strong><em>. </em><strong></strong> <strong>LARS</strong> (const bool <strong>useCholesky</strong>, const arma::mat &gramMatrix, const double <strong>lambda1</strong>=0.0, const double <strong>lambda2</strong>=0.0, const double <strong>tolerance</strong>=1e-16)</p><p><em>Set the parameters to </em><strong>LARS</strong><em>, and pass in a precalculated Gram matrix. </em><strong></strong> const std::vector&lt; size_t &gt; & <strong>ActiveSet</strong> () const </p><p><em>Access the set of active dimensions. </em> const std::vector&lt; arma::vec &gt; & <strong>BetaPath</strong> () const </p><p><em>Access the set of coefficients after each iteration; the solution is the last element. </em> const std::vector&lt; double &gt; & <strong>LambdaPath</strong> () const </p><p><em>Access the set of values for lambda1 after each iteration; the solution is the last element. </em> const arma::mat & <strong>MatUtriCholFactor</strong> () const </p><p><em>Access the upper triangular cholesky factor. </em> void <strong>Regress</strong> (const arma::mat &data, const arma::vec &responses, arma::vec &beta, const bool transposeData=true)</p><p><em>Run </em><strong>LARS</strong><em>. </em><strong></strong> std::string <strong>ToString</strong> () const </p>
<h3>Private Member Functions</h3>
<p>void <strong>Activate</strong> (const size_t varInd)</p><p><em>Add dimension varInd to active set. </em> void <strong>CholeskyDelete</strong> (const size_t colToKill)</p><p>void <strong>CholeskyInsert</strong> (const arma::vec &newX, const arma::mat &X)</p><p>void <strong>CholeskyInsert</strong> (double sqNormNewX, const arma::vec &newGramCol)</p><p>void <strong>ComputeYHatDirection</strong> (const arma::mat &matX, const arma::vec &betaDirection, arma::vec &yHatDirection)</p><p>void <strong>Deactivate</strong> (const size_t activeVarInd)</p><p><em>Remove activeVarInd'th element from active set. </em> void <strong>GivensRotate</strong> (const arma::vec::fixed&lt; 2 &gt; &x, arma::vec::fixed&lt; 2 &gt; &rotatedX, arma::mat &G)</p><p>void <strong>Ignore</strong> (const size_t varInd)</p><p><em>Add dimension varInd to ignores set (never removed). </em> void <strong>InterpolateBeta</strong> ()</p>
<h3>Private Attributes</h3>
<p>std::vector&lt; size_t &gt; <strong>activeSet</strong></p><p><em>Active set of dimensions. </em> std::vector&lt; arma::vec &gt; <strong>betaPath</strong></p><p><em>Solution path. </em> bool <strong>elasticNet</strong></p><p><em>True if this is the elastic net problem. </em> std::vector&lt; size_t &gt; <strong>ignoreSet</strong></p><p><em>Set of ignored variables (for dimensions in span{active set dimensions}). </em> std::vector&lt; bool &gt; <strong>isActive</strong></p><p><em>Active set membership indicator (for each dimension). </em> std::vector&lt; bool &gt; <strong>isIgnored</strong></p><p><em>Membership indicator for set of ignored variables. </em> double <strong>lambda1</strong></p><p><em>Regularization parameter for l1 penalty. </em> double <strong>lambda2</strong></p><p><em>Regularization parameter for l2 penalty. </em> std::vector&lt; double &gt; <strong>lambdaPath</strong></p><p><em>Value of lambda_1 for each solution in solution path. </em> bool <strong>lasso</strong></p><p><em>True if this is the LASSO problem. </em> const arma::mat & <strong>matGram</strong></p><p><em>Reference to the Gram matrix we will use. </em> arma::mat <strong>matGramInternal</strong></p><p><em>Gram matrix. </em> arma::mat <strong>matUtriCholFactor</strong></p><p><em>Upper triangular cholesky factor; initially 0x0 matrix. </em> double <strong>tolerance</strong></p><p><em>Tolerance for main loop. </em> bool <strong>useCholesky</strong></p><p><em>Whether or not to use Cholesky decomposition when solving linear system. </em></p>

        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">Detailed Description</h2>
        <div class="sectioncontent">
<p>An implementation of <strong>LARS</strong>, a stage-wise homotopy-based algorithm for l1-regularized linear regression (LASSO) and l1+l2 regularized linear regression (Elastic Net).</p><p>Let $ X $ be a matrix where each row is a point and each column is a dimension and let $ y $ be a vector of responses.</p><p>The Elastic Net problem is to solve</p><p>\[ \min_{\beta} 0.5 || X \beta - y ||_2^2 + \lambda_1 || \beta ||_1 + 0.5 \lambda_2 || \beta ||_2^2 \].PP where $ \beta $ is the vector of regression coefficients.</p><p>If $ \lambda_1 &gt; 0 $ and $ \lambda_2 = 0 $, the problem is the LASSO. If $ \lambda_1 &gt; 0 $ and $ \lambda_2 &gt; 0 $, the problem is the elastic net. If $ \lambda_1 = 0 $ and $ \lambda_2 &gt; 0 $, the problem is ridge regression. If $ \lambda_1 = 0 $ and $ \lambda_2 = 0 $, the problem is unregularized linear regression.</p><p>Note: This algorithm is not recommended for use (in terms of efficiency) when $ \lambda_1 $ = 0.</p><p>For more details, see the following papers:</p>
<pre>
@article{efron2004least,
  title={Least angle regression},
  author={Efron, B. and Hastie, T. and Johnstone, I. and Tibshirani, R.},
  journal={The Annals of statistics},
  volume={32},
  number={2},
  pages={407--499},
  year={2004},
  publisher={Institute of Mathematical Statistics}
}
</pre>

<pre>
@article{zou2005regularization,
  title={Regularization and variable selection via the elastic net},
  author={Zou, H. and Hastie, T.},
  journal={Journal of the Royal Statistical Society Series B},
  volume={67},
  number={2},
  pages={301--320},
  year={2005},
  publisher={Royal Statistical Society}
}
</pre>
<p>Definition at line 99 of file lars.hpp.</p>
        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">Constructor & Destructor Documentation</h2>
        <div class="sectioncontent">
<h3>mlpack::regression::LARS::LARS (const booluseCholesky, const doublelambda1 = \fC0.0\fP, const doublelambda2 = \fC0.0\fP, const doubletolerance = \fC1e-16\fP)</h3>
<p>Set the parameters to <strong>LARS</strong>. Both lambda1 and lambda2 default to 0.</p><p><strong>Parameters:</strong></p><p><em>useCholesky</em> Whether or not to use Cholesky decomposition when solving linear system (as opposed to using the full Gram matrix).</p><p><em>lambda1</em> Regularization parameter for l1-norm penalty.</p><p><em>lambda2</em> Regularization parameter for l2-norm penalty.</p><p><em>tolerance</em> Run until the maximum correlation of elements in (X^T y) is less than this.</p>
<h3>mlpack::regression::LARS::LARS (const booluseCholesky, const arma::mat &gramMatrix, const doublelambda1 = \fC0.0\fP, const doublelambda2 = \fC0.0\fP, const doubletolerance = \fC1e-16\fP)</h3>
<p>Set the parameters to <strong>LARS</strong>, and pass in a precalculated Gram matrix. Both lambda1 and lambda2 default to 0.</p><p><strong>Parameters:</strong></p><p><em>useCholesky</em> Whether or not to use Cholesky decomposition when solving linear system (as opposed to using the full Gram matrix).</p><p><em>gramMatrix</em> Gram matrix.</p><p><em>lambda1</em> Regularization parameter for l1-norm penalty.</p><p><em>lambda2</em> Regularization parameter for l2-norm penalty.</p><p><em>tolerance</em> Run until the maximum correlation of elements in (X^T y) is less than this.</p>

        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">Member Function Documentation</h2>
        <div class="sectioncontent">
<h3>void mlpack::regression::LARS::Activate (const size_tvarInd)\fC [private]\fP</h3>
<p>Add dimension varInd to active set.</p><p><strong>Parameters:</strong></p><p><em>varInd</em> Dimension to add to active set.</p>
<h3>const std::vector&lt;size_t&gt;& mlpack::regression::LARS::ActiveSet () const\fC [inline]\fP</h3>
<p>Access the set of active dimensions.</p><p>Definition at line 155 of file lars.hpp.</p><p>References activeSet.</p>
<h3>const std::vector&lt;arma::vec&gt;& mlpack::regression::LARS::BetaPath () const\fC [inline]\fP</h3>
<p>Access the set of coefficients after each iteration; the solution is the last element.</p><p>Definition at line 159 of file lars.hpp.</p><p>References betaPath.</p>
<h3>void mlpack::regression::LARS::CholeskyDelete (const size_tcolToKill)\fC [private]\fP</h3>

<h3>void mlpack::regression::LARS::CholeskyInsert (const arma::vec &newX, const arma::mat &X)\fC [private]\fP</h3>

<h3>void mlpack::regression::LARS::CholeskyInsert (doublesqNormNewX, const arma::vec &newGramCol)\fC [private]\fP</h3>

<h3>void mlpack::regression::LARS::ComputeYHatDirection (const arma::mat &matX, const arma::vec &betaDirection, arma::vec &yHatDirection)\fC [private]\fP</h3>

<h3>void mlpack::regression::LARS::Deactivate (const size_tactiveVarInd)\fC [private]\fP</h3>
<p>Remove activeVarInd'th element from active set.</p><p><strong>Parameters:</strong></p><p><em>activeVarInd</em> Index of element to remove from active set.</p>
<h3>void mlpack::regression::LARS::GivensRotate (const arma::vec::fixed&lt; 2 &gt; &x, arma::vec::fixed&lt; 2 &gt; &rotatedX, arma::mat &G)\fC [private]\fP</h3>

<h3>void mlpack::regression::LARS::Ignore (const size_tvarInd)\fC [private]\fP</h3>
<p>Add dimension varInd to ignores set (never removed).</p><p><strong>Parameters:</strong></p><p><em>varInd</em> Dimension to add to ignores set.</p>
<h3>void mlpack::regression::LARS::InterpolateBeta ()\fC [private]\fP</h3>

<h3>const std::vector&lt;double&gt;& mlpack::regression::LARS::LambdaPath () const\fC [inline]\fP</h3>
<p>Access the set of values for lambda1 after each iteration; the solution is the last element.</p><p>Definition at line 163 of file lars.hpp.</p><p>References lambdaPath.</p>
<h3>const arma::mat& mlpack::regression::LARS::MatUtriCholFactor () const\fC [inline]\fP</h3>
<p>Access the upper triangular cholesky factor.</p><p>Definition at line 166 of file lars.hpp.</p><p>References matUtriCholFactor.</p>
<h3>void mlpack::regression::LARS::Regress (const arma::mat &data, const arma::vec &responses, arma::vec &beta, const booltransposeData = \fCtrue\fP)</h3>
<p>Run <strong>LARS</strong>. The input matrix (like all MLPACK matrices) should be column-major -- each column is an observation and each row is a dimension. However, because <strong>LARS</strong> is more efficient on a row-major matrix, this method will (internally) transpose the matrix. If this transposition is not necessary (i.e., you want to pass in a row-major matrix), pass 'false' for the transposeData parameter.</p><p><strong>Parameters:</strong></p><p><em>data</em> Column-major input data (or row-major input data if rowMajor = true).</p><p><em>responses</em> A vector of targets.</p><p><em>beta</em> Vector to store the solution (the coefficients) in.</p><p><em>rowMajor</em> Set to false if the data is row-major.</p>
<h3>std::string mlpack::regression::LARS::ToString () const</h3>


        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">Member Data Documentation</h2>
        <div class="sectioncontent">
<h3>std::vector&lt;size_t&gt; mlpack::regression::LARS::activeSet\fC [private]\fP</h3>
<p>Active set of dimensions.</p><p>Definition at line 204 of file lars.hpp.</p><p>Referenced by ActiveSet().</p>
<h3>std::vector&lt;arma::vec&gt; mlpack::regression::LARS::betaPath\fC [private]\fP</h3>
<p>Solution path.</p><p>Definition at line 198 of file lars.hpp.</p><p>Referenced by BetaPath().</p>
<h3>bool mlpack::regression::LARS::elasticNet\fC [private]\fP</h3>
<p>True if this is the elastic net problem.</p><p>Definition at line 190 of file lars.hpp.</p>
<h3>std::vector&lt;size_t&gt; mlpack::regression::LARS::ignoreSet\fC [private]\fP</h3>
<p>Set of ignored variables (for dimensions in span{active set dimensions}).</p><p>Definition at line 212 of file lars.hpp.</p>
<h3>std::vector&lt;bool&gt; mlpack::regression::LARS::isActive\fC [private]\fP</h3>
<p>Active set membership indicator (for each dimension).</p><p>Definition at line 207 of file lars.hpp.</p>
<h3>std::vector&lt;bool&gt; mlpack::regression::LARS::isIgnored\fC [private]\fP</h3>
<p>Membership indicator for set of ignored variables.</p><p>Definition at line 215 of file lars.hpp.</p>
<h3>double mlpack::regression::LARS::lambda1\fC [private]\fP</h3>
<p>Regularization parameter for l1 penalty.</p><p>Definition at line 187 of file lars.hpp.</p>
<h3>double mlpack::regression::LARS::lambda2\fC [private]\fP</h3>
<p>Regularization parameter for l2 penalty.</p><p>Definition at line 192 of file lars.hpp.</p>
<h3>std::vector&lt;double&gt; mlpack::regression::LARS::lambdaPath\fC [private]\fP</h3>
<p>Value of lambda_1 for each solution in solution path.</p><p>Definition at line 201 of file lars.hpp.</p><p>Referenced by LambdaPath().</p>
<h3>bool mlpack::regression::LARS::lasso\fC [private]\fP</h3>
<p>True if this is the LASSO problem.</p><p>Definition at line 185 of file lars.hpp.</p>
<h3>const arma::mat& mlpack::regression::LARS::matGram\fC [private]\fP</h3>
<p>Reference to the Gram matrix we will use.</p><p>Definition at line 176 of file lars.hpp.</p>
<h3>arma::mat mlpack::regression::LARS::matGramInternal\fC [private]\fP</h3>
<p>Gram matrix.</p><p>Definition at line 173 of file lars.hpp.</p>
<h3>arma::mat mlpack::regression::LARS::matUtriCholFactor\fC [private]\fP</h3>
<p>Upper triangular cholesky factor; initially 0x0 matrix.</p><p>Definition at line 179 of file lars.hpp.</p><p>Referenced by MatUtriCholFactor().</p>
<h3>double mlpack::regression::LARS::tolerance\fC [private]\fP</h3>
<p>Tolerance for main loop.</p><p>Definition at line 195 of file lars.hpp.</p>
<h3>bool mlpack::regression::LARS::useCholesky\fC [private]\fP</h3>
<p>Whether or not to use Cholesky decomposition when solving linear system.</p><p>Definition at line 182 of file lars.hpp.</p>

        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">Author</h2>
        <div class="sectioncontent">
<p>Generated automatically by Doxygen for MLPACK from the source code.</p>
        </div>
      </section>
<nav>
  <ul class="pager">
   <li class="previous"><a href="mlpack_regression.3.html"><span aria-hidden="true">&larr;</span> mlpack_regression.3: Regression methods.</a></li>
   <li class="next"><a href="mlpack_regression_LinearRegression.3.html">mlpack_regression_LinearRegression.3: A simple linear regression algorithm using ordinary least squares. <span aria-hidden="true">&rarr;</span></a></li>
  </ul>
</nav>

  </div>
  <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
  <!-- Include all compiled plugins (below), or include individual files as needed -->
  <script src="/js/bootstrap.min.js"></script>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-60781387-1', 'auto');
    ga('send', 'pageview');

  </script>
</body>
</html>
