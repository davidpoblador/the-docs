<!DOCTYPE html>
<html lang="en">
<head>
  <link href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700&effect=destruction%7Cshadow-multiple" rel="stylesheet" type="text/css">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>mlpack_optimization_SGD: Stochastic gradient descent is a technique for minimizing a function which can be expressed as a sum of other functions.</title>
  <link href="/css/bootstrap.min.css" rel="stylesheet">
  <link href="/css/manpage.css" rel="stylesheet">
  <link rel="apple-touch-icon" sizes="57x57" href="/icons/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/icons/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/icons/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/icons/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/icons/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/icons/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/icons/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/icons/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192" href="/icons/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/icons/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png">
  <link rel="manifest" href="/icons/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/icons/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">
  <meta name="description" content="Stochastic gradient descent is a technique for minimizing a function which can be expressed as a sum of other functions.">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@CartaTech">
  <meta name="twitter:creator" content="@CartaTech">
  <meta name="twitter:title" content="mlpack_optimization_SGD (3) manual">
  <meta name="twitter:description" content="Stochastic gradient descent is a technique for minimizing a function which can be expressed as a sum of other functions.">
  <meta name="twitter:image" content="https://www.carta.tech/images/mlpack-doc-mlpack_optimization_SGD-3.png">
  <meta property="og:url" content="https://www.carta.tech/man-pages/man3/mlpack_optimization_SGD.3.html" />
  <meta property="og:type" content="website" />
  <meta property="og:title" content="mlpack_optimization_SGD (3) manual" />
  <meta property="og:description" content="Stochastic gradient descent is a technique for minimizing a function which can be expressed as a sum of other functions." />
  <meta property="fb:app_id" content="1241677679199500" />
  <meta property="og:image" content="https://www.carta.tech/images/mlpack-doc-mlpack_optimization_SGD-3.png" />
  <meta property="og:image:width" content="600" />
  <meta property="og:image:height" content="315" />
</head>
<body>
  <div class="container final">
          <div class="page-header">
        <h1 class="font-effect-destruction">mlpack_optimization_SGD<small> (3)</small></h1>
        <p class="lead">Stochastic gradient descent is a technique for minimizing a function which can be expressed as a sum of other functions.</p>
      </div>

    <ol class="breadcrumb" itemscope itemtype="http://schema.org/BreadcrumbList">
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/">
      <span itemprop="name">Carta.tech</span>
    </a>
    <meta itemprop="position" content="1" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/man-pages/">
      <span itemprop="name">Man Pages</span>
    </a>
    <meta itemprop="position" content="2" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/man-pages/man3/">
      <span itemprop="name">Library calls</span>
    </a>
    <meta itemprop="position" content="3" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/man-pages/man3/mlpack_optimization_SGD.3.html">
      <span itemprop="name">mlpack_optimization_SGD: Stochastic gradient descent is a technique for minimizing a function which can be expressed as a sum of other functions.</span>
    </a>
    <meta itemprop="position" content="4" />
  </li>
</ol>
<ol class="breadcrumb" itemscope itemtype="http://schema.org/BreadcrumbList">
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/">
      <span itemprop="name">Carta.tech</span>
    </a>
    <meta itemprop="position" content="1" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/packages/">
      <span itemprop="name">Packages</span>
    </a>
    <meta itemprop="position" content="2" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/packages/mlpack-doc/">
      <span itemprop="name">mlpack-doc</span>
    </a>
    <meta itemprop="position" content="3" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/man-pages/man3/mlpack_optimization_SGD.3.html">
      <span itemprop="name">mlpack_optimization_SGD: Stochastic gradient descent is a technique for minimizing a function which can be expressed as a sum of other functions.</span>
    </a>
    <meta itemprop="position" content="4" />
  </li>
</ol>
    
      <section>
        <h2 class="font-effect-shadow-multiple">SYNOPSIS</h2>
        <div class="sectioncontent">
<h3>Public Member Functions</h3>
<p><strong>SGD</strong> (DecomposableFunctionType &<strong>function</strong>, const double <strong>stepSize</strong>=0.01, const size_t <strong>maxIterations</strong>=100000, const double <strong>tolerance</strong>=1e-5, const bool shuffle=true)</p><p><em>Construct the </em><strong>SGD</strong><em> optimizer with the given function and parameters. </em><strong></strong> const DecomposableFunctionType & <strong>Function</strong> () const </p><p><em>Get the instantiated function to be optimized. </em> DecomposableFunctionType & <strong>Function</strong> ()</p><p><em>Modify the instantiated function. </em> size_t <strong>MaxIterations</strong> () const </p><p><em>Get the maximum number of iterations (0 indicates no limit). </em> size_t & <strong>MaxIterations</strong> ()</p><p><em>Modify the maximum number of iterations (0 indicates no limit). </em> double <strong>Optimize</strong> (arma::mat &iterate)</p><p><em>Optimize the given function using stochastic gradient descent. </em> template&lt;&gt; double <strong>Optimize</strong> (arma::mat &parameters)</p><p>bool <strong>Shuffle</strong> () const </p><p><em>Get whether or not the individual functions are shuffled. </em> bool & <strong>Shuffle</strong> ()</p><p><em>Modify whether or not the individual functions are shuffled. </em> double <strong>StepSize</strong> () const </p><p><em>Get the step size. </em> double & <strong>StepSize</strong> ()</p><p><em>Modify the step size. </em> double <strong>Tolerance</strong> () const </p><p><em>Get the tolerance for termination. </em> double & <strong>Tolerance</strong> ()</p><p><em>Modify the tolerance for termination. </em> std::string <strong>ToString</strong> () const </p>
<h3>Private Attributes</h3>
<p>DecomposableFunctionType & <strong>function</strong></p><p><em>The instantiated function. </em> size_t <strong>maxIterations</strong></p><p><em>The maximum number of allowed iterations. </em> bool <strong>shuffle</strong></p><p><em>Controls whether or not the individual functions are shuffled when iterating. </em> double <strong>stepSize</strong></p><p><em>The step size for each example. </em> double <strong>tolerance</strong></p><p><em>The tolerance for termination. </em></p>

        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">Detailed Description</h2>
        <div class="sectioncontent">
<h3>template&lt;typename DecomposableFunctionType&gt;class mlpack::optimization::SGD&lt; DecomposableFunctionType &gt;</h3>
<p>Stochastic Gradient Descent is a technique for minimizing a function which can be expressed as a sum of other functions.</p><p>That is, suppose we have</p><p>\[ f(A) = \sum_{i = 0}^{n} f_i(A) \].PP and our task is to minimize $ A $. Stochastic gradient descent iterates over each function $ f_i(A) $, producing the following update scheme:</p><p>\[ A_{j + 1} = A_j + \alpha \nabla f_i(A) \].PP where $ \alpha $ is a parameter which specifies the step size. $ i $ is chosen according to $ j $ (the iteration number). The <strong>SGD</strong> class supports either scanning through each of the $ n $ functions $ f_i(A) $ linearly, or in a random sequence. The algorithm continues until $ j $ reaches the maximum number of iterations -- or when a full sequence of updates through each of the $ n $ functions $ f_i(A) $ produces an improvement within a certain tolerance $ &#92;psilon $. That is,</p><p>\[ | f(A_{j + n}) - f(A_j) | &lt; &#92;psilon. \].PP The parameter $&#92;psilon$ is specified by the tolerance parameter to the constructor; $n$ is specified by the maxIterations parameter.</p><p>This class is useful for data-dependent functions whose objective function can be expressed as a sum of objective functions operating on an individual point. Then, <strong>SGD</strong> considers the gradient of the objective function operating on an individual point in its update of $ A $.</p><p>For <strong>SGD</strong> to work, a DecomposableFunctionType template parameter is required. This class must implement the following function:</p><p>size_t NumFunctions(); double Evaluate(const arma::mat& coordinates, const size_t i); void Gradient(const arma::mat& coordinates, const size_t i, arma::mat& gradient);</p><p>NumFunctions() should return the number of functions ( $n$), and in the other two functions, the parameter i refers to which individual function (or gradient) is being evaluated. So, for the case of a data-dependent function, such as NCA (see <strong>mlpack::nca::NCA</strong>), NumFunctions() should return the number of points in the dataset, and Evaluate(coordinates, 0) will evaluate the objective function on the first point in the dataset (presumably, the dataset is held internally in the DecomposableFunctionType).</p><p><strong>Template Parameters:</strong></p><p><em>DecomposableFunctionType</em> Decomposable objective function type to be minimized.</p><p>Definition at line 86 of file sgd.hpp.</p>

        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">Constructor & Destructor Documentation</h2>
        <div class="sectioncontent">
<h3>template&lt;typename DecomposableFunctionType&gt; \fBmlpack::optimization::SGD\fP&lt; DecomposableFunctionType &gt;::\fBSGD\fP (DecomposableFunctionType &function, const doublestepSize = \fC0.01\fP, const size_tmaxIterations = \fC100000\fP, const doubletolerance = \fC1e-5\fP, const boolshuffle = \fCtrue\fP)</h3>
<p>Construct the <strong>SGD</strong> optimizer with the given function and parameters.</p><p><strong>Parameters:</strong></p><p><em>function</em> Function to be optimized (minimized).</p><p><em>stepSize</em> Step size for each iteration.</p><p><em>maxIterations</em> Maximum number of iterations allowed (0 means no limit).</p><p><em>tolerance</em> Maximum absolute tolerance to terminate algorithm.</p><p><em>shuffle</em> If true, the function order is shuffled; otherwise, each function is visited in linear order.</p>

        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">Member Function Documentation</h2>
        <div class="sectioncontent">
<h3>template&lt;typename DecomposableFunctionType&gt; const DecomposableFunctionType& \fBmlpack::optimization::SGD\fP&lt; DecomposableFunctionType &gt;::Function () const\fC [inline]\fP</h3>
<p>Get the instantiated function to be optimized.</p><p>Definition at line 117 of file sgd.hpp.</p>
<h3>template&lt;typename DecomposableFunctionType&gt; DecomposableFunctionType& \fBmlpack::optimization::SGD\fP&lt; DecomposableFunctionType &gt;::Function ()\fC [inline]\fP</h3>
<p>Modify the instantiated function.</p><p>Definition at line 119 of file sgd.hpp.</p>
<h3>template&lt;typename DecomposableFunctionType&gt; size_t \fBmlpack::optimization::SGD\fP&lt; DecomposableFunctionType &gt;::MaxIterations () const\fC [inline]\fP</h3>
<p>Get the maximum number of iterations (0 indicates no limit).</p><p>Definition at line 127 of file sgd.hpp.</p>
<h3>template&lt;typename DecomposableFunctionType&gt; size_t& \fBmlpack::optimization::SGD\fP&lt; DecomposableFunctionType &gt;::MaxIterations ()\fC [inline]\fP</h3>
<p>Modify the maximum number of iterations (0 indicates no limit).</p><p>Definition at line 129 of file sgd.hpp.</p>
<h3>template&lt;typename DecomposableFunctionType&gt; double \fBmlpack::optimization::SGD\fP&lt; DecomposableFunctionType &gt;::Optimize (arma::mat &iterate)</h3>
<p>Optimize the given function using stochastic gradient descent. The given starting point will be modified to store the finishing point of the algorithm, and the final objective value is returned.</p><p><strong>Parameters:</strong></p><p><em>iterate</em> Starting point (will be modified).</p><p><strong>Returns:</strong></p><p>Objective value of the final point.</p>
<h3>template&lt;&gt; double \fBmlpack::optimization::SGD\fP&lt; \fBmlpack::svd::RegularizedSVDFunction\fP &gt;::Optimize (arma::mat &parameters)</h3>
<p>Used because the gradient affects only a small number of parameters per example, and thus the normal abstraction does not work as fast as we might like it to.</p>
<h3>template&lt;typename DecomposableFunctionType&gt; bool \fBmlpack::optimization::SGD\fP&lt; DecomposableFunctionType &gt;::Shuffle () const\fC [inline]\fP</h3>
<p>Get whether or not the individual functions are shuffled.</p><p>Definition at line 137 of file sgd.hpp.</p>
<h3>template&lt;typename DecomposableFunctionType&gt; bool& \fBmlpack::optimization::SGD\fP&lt; DecomposableFunctionType &gt;::Shuffle ()\fC [inline]\fP</h3>
<p>Modify whether or not the individual functions are shuffled.</p><p>Definition at line 139 of file sgd.hpp.</p>
<h3>template&lt;typename DecomposableFunctionType&gt; double \fBmlpack::optimization::SGD\fP&lt; DecomposableFunctionType &gt;::StepSize () const\fC [inline]\fP</h3>
<p>Get the step size.</p><p>Definition at line 122 of file sgd.hpp.</p>
<h3>template&lt;typename DecomposableFunctionType&gt; double& \fBmlpack::optimization::SGD\fP&lt; DecomposableFunctionType &gt;::StepSize ()\fC [inline]\fP</h3>
<p>Modify the step size.</p><p>Definition at line 124 of file sgd.hpp.</p>
<h3>template&lt;typename DecomposableFunctionType&gt; double \fBmlpack::optimization::SGD\fP&lt; DecomposableFunctionType &gt;::Tolerance () const\fC [inline]\fP</h3>
<p>Get the tolerance for termination.</p><p>Definition at line 132 of file sgd.hpp.</p>
<h3>template&lt;typename DecomposableFunctionType&gt; double& \fBmlpack::optimization::SGD\fP&lt; DecomposableFunctionType &gt;::Tolerance ()\fC [inline]\fP</h3>
<p>Modify the tolerance for termination.</p><p>Definition at line 134 of file sgd.hpp.</p>
<h3>template&lt;typename DecomposableFunctionType&gt; std::string \fBmlpack::optimization::SGD\fP&lt; DecomposableFunctionType &gt;::ToString () const</h3>


        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">Member Data Documentation</h2>
        <div class="sectioncontent">
<h3>template&lt;typename DecomposableFunctionType&gt; DecomposableFunctionType& \fBmlpack::optimization::SGD\fP&lt; DecomposableFunctionType &gt;::function\fC [private]\fP</h3>
<p>The instantiated function.</p><p>Definition at line 146 of file sgd.hpp.</p>
<h3>template&lt;typename DecomposableFunctionType&gt; size_t \fBmlpack::optimization::SGD\fP&lt; DecomposableFunctionType &gt;::maxIterations\fC [private]\fP</h3>
<p>The maximum number of allowed iterations.</p><p>Definition at line 152 of file sgd.hpp.</p><p>Referenced by mlpack::optimization::SGD&lt; mlpack::svd::RegularizedSVDFunction &gt;::MaxIterations().</p>
<h3>template&lt;typename DecomposableFunctionType&gt; bool \fBmlpack::optimization::SGD\fP&lt; DecomposableFunctionType &gt;::shuffle\fC [private]\fP</h3>
<p>Controls whether or not the individual functions are shuffled when iterating.</p><p>Definition at line 159 of file sgd.hpp.</p><p>Referenced by mlpack::optimization::SGD&lt; mlpack::svd::RegularizedSVDFunction &gt;::Shuffle().</p>
<h3>template&lt;typename DecomposableFunctionType&gt; double \fBmlpack::optimization::SGD\fP&lt; DecomposableFunctionType &gt;::stepSize\fC [private]\fP</h3>
<p>The step size for each example.</p><p>Definition at line 149 of file sgd.hpp.</p><p>Referenced by mlpack::optimization::SGD&lt; mlpack::svd::RegularizedSVDFunction &gt;::StepSize().</p>
<h3>template&lt;typename DecomposableFunctionType&gt; double \fBmlpack::optimization::SGD\fP&lt; DecomposableFunctionType &gt;::tolerance\fC [private]\fP</h3>
<p>The tolerance for termination.</p><p>Definition at line 155 of file sgd.hpp.</p><p>Referenced by mlpack::optimization::SGD&lt; mlpack::svd::RegularizedSVDFunction &gt;::Tolerance().</p>

        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">Author</h2>
        <div class="sectioncontent">
<p>Generated automatically by Doxygen for MLPACK from the source code.</p>
        </div>
      </section>
<nav>
  <ul class="pager">
   <li class="previous"><a href="mlpack_optimization_SA.3.html"><span aria-hidden="true">&larr;</span> mlpack_optimization_SA.3: Simulated annealing is an stochastic optimization algorithm which is able to deliver near-optimal results quickly without knowing the gradient of the function being optimized.</a></li>
   <li class="next"><a href="mlpack_optimization_test.3.html">mlpack_optimization_test.3: Mlpack::optimization::test - <span aria-hidden="true">&rarr;</span></a></li>
  </ul>
</nav>

  </div>
  <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
  <!-- Include all compiled plugins (below), or include individual files as needed -->
  <script src="/js/bootstrap.min.js"></script>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-60781387-1', 'auto');
    ga('send', 'pageview');

  </script>
</body>
</html>
