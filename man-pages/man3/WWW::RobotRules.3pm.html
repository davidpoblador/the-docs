<!DOCTYPE html>
<html lang="en">
<head>
  <link href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700&effect=destruction%7Cshadow-multiple" rel="stylesheet" type="text/css">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>WWW::RobotRules: Database of robots.txt-derived permissions</title>
  <link href="/css/bootstrap.min.css" rel="stylesheet">
  <link href="/css/manpage.css" rel="stylesheet">
  <link rel="apple-touch-icon" sizes="57x57" href="/icons/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/icons/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/icons/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/icons/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/icons/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/icons/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/icons/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/icons/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192" href="/icons/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/icons/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png">
  <link rel="manifest" href="/icons/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/icons/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">
  <meta name="description" content="Database of robots.txt-derived permissions">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@CartaTech">
  <meta name="twitter:creator" content="@CartaTech">
  <meta name="twitter:title" content="WWW::RobotRules (3pm) manual">
  <meta name="twitter:description" content="Database of robots.txt-derived permissions">
  <meta name="twitter:image" content="https://www.carta.tech/images/libwww-robotrules-perl-WWW::RobotRules-3pm.png">
  <meta property="og:url" content="https://www.carta.tech/man-pages/man3pm/WWW::RobotRules.3pm.html" />
  <meta property="og:type" content="website" />
  <meta property="og:title" content="WWW::RobotRules (3pm) manual" />
  <meta property="og:description" content="Database of robots.txt-derived permissions" />
  <meta property="fb:app_id" content="1241677679199500" />
  <meta property="og:image" content="https://www.carta.tech/images/libwww-robotrules-perl-WWW::RobotRules-3pm.png" />
  <meta property="og:image:width" content="600" />
  <meta property="og:image:height" content="315" />
</head>
<body>
  <div class="container final">
          <div class="page-header">
        <h1 class="font-effect-destruction">WWW::RobotRules<small> (3pm)</small></h1>
        <p class="lead">Database of robots.txt-derived permissions</p>
      </div>

    <ol class="breadcrumb" itemscope itemtype="http://schema.org/BreadcrumbList">
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/">
      <span itemprop="name">Carta.tech</span>
    </a>
    <meta itemprop="position" content="1" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/man-pages/">
      <span itemprop="name">Man Pages</span>
    </a>
    <meta itemprop="position" content="2" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/man-pages/man3pm/">
      <span itemprop="name">MISSING SECTION</span>
    </a>
    <meta itemprop="position" content="3" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/man-pages/man3pm/WWW::RobotRules.3pm.html">
      <span itemprop="name">WWW::RobotRules: Database of robots.txt-derived permissions</span>
    </a>
    <meta itemprop="position" content="4" />
  </li>
</ol>
<ol class="breadcrumb" itemscope itemtype="http://schema.org/BreadcrumbList">
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/">
      <span itemprop="name">Carta.tech</span>
    </a>
    <meta itemprop="position" content="1" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/packages/">
      <span itemprop="name">Packages</span>
    </a>
    <meta itemprop="position" content="2" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/packages/libwww-robotrules-perl/">
      <span itemprop="name">libwww-robotrules-perl</span>
    </a>
    <meta itemprop="position" content="3" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/man-pages/man3pm/WWW::RobotRules.3pm.html">
      <span itemprop="name">WWW::RobotRules: Database of robots.txt-derived permissions</span>
    </a>
    <meta itemprop="position" content="4" />
  </li>
</ol>
    
      <section>
        <h2 class="font-effect-shadow-multiple">SYNOPSIS</h2>
        <div class="sectioncontent">

<pre>
 use WWW::RobotRules;
 my $rules = WWW::RobotRules-&gt;new(&apos;MOMspider/1.0&apos;);

 use LWP::Simple qw(get);

 {
   my $url = "http://some.place/robots.txt";
   my $robots_txt = get $url;
   $rules-&gt;parse($url, $robots_txt) if defined $robots_txt;
 }

 {
   my $url = "http://some.other.place/robots.txt";
   my $robots_txt = get $url;
   $rules-&gt;parse($url, $robots_txt) if defined $robots_txt;
 }

 # Now we can check if a URL is valid for those servers
 # whose "robots.txt" files we&apos;ve gotten and parsed:
 if($rules-&gt;allowed($url)) {
     $c = get $url;
     ...
 }
</pre>

        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">DESCRIPTION</h2>
        <div class="sectioncontent">
<p>This module parses <em>/robots.txt</em> files as specified in \*(L"A Standard for Robot Exclusion\*(R", at &lt;http://www.robotstxt.org/wc/norobots.html&gt; Webmasters can use the <em>/robots.txt</em> file to forbid conforming robots from accessing parts of their web site.</p><p>The parsed files are kept in a WWW::RobotRules object, and this object provides methods to check if access to a given \s-1URL\s0 is prohibited.  The same WWW::RobotRules object can be used for one or more parsed <em>/robots.txt</em> files on any number of hosts.</p><p>The following methods are provided: This is the constructor for WWW::RobotRules objects.  The first argument given to <em>new()</em> is the name of the robot. The <em>parse()</em> method takes as arguments the \s-1URL\s0 that was used to retrieve the <em>/robots.txt</em> file, and the contents of the file. Returns \s-1TRUE\s0 if this robot is allowed to retrieve this \s-1URL\s0. Get/set the agent name. \s-1NOTE:\s0 Changing the agent name will clear the robots.txt rules and expire times out of the cache.</p>
        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">ROBOTS.TXT</h2>
        <div class="sectioncontent">
<p>The format and semantics of the \*(L"/robots.txt\*(R" file are as follows (this is an edited abstract of &lt;http://www.robotstxt.org/wc/norobots.html&gt;):</p><p>The file consists of one or more records separated by one or more blank lines. Each record contains lines of the form</p><p>  &lt;field-name&gt;: &lt;value&gt;</p><p>The field name is case insensitive.  Text after the '#' character on a line is ignored during parsing.  This is used for comments.  The following &lt;field-names&gt; can be used:</p>
<dl class='dl-vertical'>
  <dt>
    User-Agent
  </dt>
  <dd>
    <p>The value of this field is the name of the robot the record is describing access policy for.  If more than one <em>User-Agent</em> field is present the record describes an identical access policy for more than one robot. At least one field needs to be present per record.  If the value is '*', the record describes the default access policy for any robot that has not not matched any of the other records. The <em>User-Agent</em> fields must occur before the <em>Disallow</em> fields.  If a record contains a <em>User-Agent</em> field after a <em>Disallow</em> field, that constitutes a malformed record.  This parser will assume that a blank line should have been placed before that <em>User-Agent</em> field, and will break the record into two.  All the fields before the <em>User-Agent</em> field will constitute a record, and the <em>User-Agent</em> field will be the first field in a new record.</p>
  </dd>
  <dt>
    Disallow
  </dt>
  <dd>
    <p>The value of this field specifies a partial \s-1URL\s0 that is not to be visited. This can be a full path, or a partial path; any \s-1URL\s0 that starts with this value will not be retrieved</p>
  </dd>

</dl>
<p>Unrecognized records are ignored.</p>
        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">ROBOTS.TXT EXAMPLES</h2>
        <div class="sectioncontent">
<p>The following example \*(L"/robots.txt\*(R" file specifies that no robots should visit any \s-1URL\s0 starting with \*(L"/cyberworld/map/\*(R" or \*(L"/tmp/\*(R":</p><p>  User-agent: *   Disallow: /cyberworld/map/ # This is an infinite virtual URL space   Disallow: /tmp/ # these will soon disappear</p><p>This example \*(L"/robots.txt\*(R" file specifies that no robots should visit any \s-1URL\s0 starting with \*(L"/cyberworld/map/\*(R", except the robot called \*(L"cybermapper\*(R":</p><p>  User-agent: *   Disallow: /cyberworld/map/ # This is an infinite virtual URL space</p><p>  # Cybermapper knows where to go.   User-agent: cybermapper   Disallow:</p><p>This example indicates that no robots should visit this site further:</p><p>  # go away   User-agent: *   Disallow: /</p><p>This is an example of a malformed robots.txt file.</p><p>  # robots.txt for ancientcastle.example.com   # I&apos;ve locked myself away.   User-agent: *   Disallow: /   # The castle is your home now, so you can go anywhere you like.   User-agent: Belle   Disallow: /west-wing/ # except the west wing!   # It&apos;s good to be the Prince...   User-agent: Beast   Disallow:</p><p>This file is missing the required blank lines between records. However, the intention is clear.</p>
        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">RELATED TO WWW::RobotRules&hellip;</h2>
        <div class="sectioncontent">
<p>LWP::RobotUA, WWW::RobotRules::AnyDBM_File</p>
        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">COPYRIGHT</h2>
        <div class="sectioncontent">
<p>  Copyright 1995-2009, Gisle Aas   Copyright 1995, Martijn Koster</p><p>This library is free software; you can redistribute it and/or modify it under the same terms as Perl itself.</p>
        </div>
      </section>
<nav>
  <ul class="pager">
   <li class="previous"><a href="WWW::OpenSearch::Url.3pm.html"><span aria-hidden="true">&larr;</span> WWW::OpenSearch::Url.3pm: Object to represent a target url</a></li>
   <li class="next"><a href="WWW::RobotRules::AnyDBM_File.3pm.html">WWW::RobotRules::AnyDBM_File.3pm: Persistent robotrules <span aria-hidden="true">&rarr;</span></a></li>
  </ul>
</nav>

  </div>
  <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
  <!-- Include all compiled plugins (below), or include individual files as needed -->
  <script src="/js/bootstrap.min.js"></script>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-60781387-1', 'auto');
    ga('send', 'pageview');

  </script>
</body>
</html>
