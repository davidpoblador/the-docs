<!DOCTYPE html>
<html lang="en">
<head>
  <link href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700&effect=destruction%7Cshadow-multiple" rel="stylesheet" type="text/css">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>test_server: This module provides support for test suite authors.</title>
  <link href="/css/bootstrap.min.css" rel="stylesheet">
  <link href="/css/manpage.css" rel="stylesheet">
  <link rel="apple-touch-icon" sizes="57x57" href="/icons/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/icons/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/icons/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/icons/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/icons/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/icons/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/icons/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/icons/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192" href="/icons/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/icons/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png">
  <link rel="manifest" href="/icons/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/icons/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">
  <meta name="description" content="This module provides support for test suite authors.">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@CartaTech">
  <meta name="twitter:creator" content="@CartaTech">
  <meta name="twitter:title" content="test_server (3erl) manual">
  <meta name="twitter:description" content="This module provides support for test suite authors.">
  <meta name="twitter:image" content="https://www.carta.tech/images/erlang-manpages-test_server-3erl.png">
  <meta property="og:url" content="https://www.carta.tech/man-pages/man3erl/test_server.3erl.html" />
  <meta property="og:type" content="website" />
  <meta property="og:title" content="test_server (3erl) manual" />
  <meta property="og:description" content="This module provides support for test suite authors." />
  <meta property="fb:app_id" content="1241677679199500" />
  <meta property="og:image" content="https://www.carta.tech/images/erlang-manpages-test_server-3erl.png" />
  <meta property="og:image:width" content="600" />
  <meta property="og:image:height" content="315" />
</head>
<body>
  <div class="container final">
          <div class="page-header">
        <h1 class="font-effect-destruction">test_server<small> (3erl)</small></h1>
        <p class="lead">This module provides support for test suite authors.</p>
      </div>

    <ol class="breadcrumb" itemscope itemtype="http://schema.org/BreadcrumbList">
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/">
      <span itemprop="name">Carta.tech</span>
    </a>
    <meta itemprop="position" content="1" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/man-pages/">
      <span itemprop="name">Man Pages</span>
    </a>
    <meta itemprop="position" content="2" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/man-pages/man3erl/">
      <span itemprop="name">MISSING SECTION</span>
    </a>
    <meta itemprop="position" content="3" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/man-pages/man3erl/test_server.3erl.html">
      <span itemprop="name">test_server: This module provides support for test suite authors.</span>
    </a>
    <meta itemprop="position" content="4" />
  </li>
</ol>
<ol class="breadcrumb" itemscope itemtype="http://schema.org/BreadcrumbList">
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/">
      <span itemprop="name">Carta.tech</span>
    </a>
    <meta itemprop="position" content="1" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/packages/">
      <span itemprop="name">Packages</span>
    </a>
    <meta itemprop="position" content="2" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/packages/erlang-manpages/">
      <span itemprop="name">erlang-manpages</span>
    </a>
    <meta itemprop="position" content="3" />
  </li>
  <li itemprop="itemListElement" itemscope itemtype="http://schema.org/ListItem">
    <a itemscope itemtype="http://schema.org/Thing" itemprop="item" href="/man-pages/man3erl/test_server.3erl.html">
      <span itemprop="name">test_server: This module provides support for test suite authors.</span>
    </a>
    <meta itemprop="position" content="4" />
  </li>
</ol>
    
      <section>
        <h2 class="font-effect-shadow-multiple">DESCRIPTION</h2>
        <div class="sectioncontent">
<p>The <em>test_server</em> module aids the test suite author by providing various support functions. The supported functionality includes:</p>
<dl class='dl-vertical'>
  <dt>
    <p>*</p>
  </dt>
  <dd>
    <p>Logging and timestamping</p>
  </dd>

</dl>

<dl class='dl-vertical'>
  <dt>
    <p>*</p>
  </dt>
  <dd>
    <p>Capturing output to stdout</p>
  </dd>

</dl>

<dl class='dl-vertical'>
  <dt>
    <p>*</p>
  </dt>
  <dd>
    <p>Retrieving and flushing the message queue of a process</p>
  </dd>

</dl>

<dl class='dl-vertical'>
  <dt>
    <p>*</p>
  </dt>
  <dd>
    <p>Watchdog timers, process sleep, time measurement and unit conversion</p>
  </dd>

</dl>

<dl class='dl-vertical'>
  <dt>
    <p>*</p>
  </dt>
  <dd>
    <p>Private scratch directory for all test suites</p>
  </dd>

</dl>

<dl class='dl-vertical'>
  <dt>
    <p>*</p>
  </dt>
  <dd>
    <p>Start and stop of slave- or peer nodes</p>
  </dd>

</dl>
<p>For more information on how to write test cases and for examples, please see the Test Server User's Guide.</p>
        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">TEST SUITE SUPPORT FUNCTIONS</h2>
        <div class="sectioncontent">
<p>The following functions are supposed to be used inside a test suite.</p>
        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">EXPORTS</h2>
        <div class="sectioncontent">
<p><strong></strong> os_type() -&gt; OSType</p><p>Types:</p><p>OSType = term()</p><p>This is the same as returned from <em>os:type/0</em></p><p>This function is equivalent to <em>os:type/0</em>. It is kept for backwards compatibility.</p><p><strong></strong> fail()</p><p><strong></strong> fail(Reason)</p><p>Types:</p><p>Reason = term()</p><p>The reason why the test case failed.</p><p>This will make the test suite fail with a given reason, or with <em>suite_failed</em> if no reason was given. Use this function if you want to terminate a test case, as this will make it easier to read the log- and HTML files. <em>Reason</em> will appear in the comment field in the HTML log.</p><p><strong></strong> timetrap(Timout) -&gt; Handle</p><p>Types:</p><p>Timeout = integer() | {hours,H} | {minutes,M} | {seconds,S}</p><p>H = M = S = integer()</p><p>Pid = pid()</p><p>The process that is to be timetrapped (<em>self()</em>by default)</p><p>Sets up a time trap for the current process. An expired timetrap kills the process with reason <em>timetrap_timeout</em>. The returned handle is to be given as argument to <em>timetrap_cancel</em> before the timetrap expires. If <em>Timeout</em> is an integer, it is expected to be milliseconds.</p><p><strong></strong> Note:</p><p>If the current process is trapping exits, it will not be killed by the exit signal with reason <em>timetrap_timeout</em>. If this happens, the process will be sent an exit signal with reason <em>kill</em> 10 seconds later which will kill the process. Information about the timetrap timeout will in this case not be found in the test logs. However, the error_logger will be sent a warning.</p><p><strong></strong> timetrap_cancel(Handle) -&gt; ok</p><p>Types:</p><p>Handle = term()</p><p>Handle returned from <em>timetrap</em></p><p>This function cancels a timetrap. This must be done before the timetrap expires.</p><p><strong></strong> timetrap_scale_factor() -&gt; ScaleFactor</p><p>Types:</p><p>ScaleFactor = integer()</p><p>This function returns the scale factor by which all timetraps are scaled. It is normally 1, but can be greater than 1 if the test_server is running <em>cover</em>, using a larger amount of scheduler threads than the amount of logical processors on the system, running under purify, valgrind or in a debug-compiled emulator. The scale factor can be used if you need to scale you own timeouts in test cases with same factor as the test_server uses.</p><p><strong></strong> sleep(MSecs) -&gt; ok</p><p>Types:</p><p>MSecs = integer() | float() | infinity</p><p>The number of milliseconds to sleep</p><p>This function suspends the calling process for at least the supplied number of milliseconds. There are two major reasons why you should use this function instead of <em>timer:sleep</em>, the first being that the module <em>timer</em> may be unavailable at the time the test suite is run, and the second that it also accepts floating point numbers.</p><p><strong></strong> adjusted_sleep(MSecs) -&gt; ok</p><p>Types:</p><p>MSecs = integer() | float() | infinity</p><p>The default number of milliseconds to sleep</p><p>This function suspends the calling process for at least the supplied number of milliseconds. The function behaves the same way as <em>test_server:sleep/1</em>, only <em>MSecs</em> will be multiplied by the 'multiply_timetraps' value, if set, and also automatically scaled up if 'scale_timetraps' is set to true (which it is by default).</p><p><strong></strong> hours(N) -&gt; MSecs</p><p><strong></strong> minutes(N) -&gt; MSecs</p><p><strong></strong> seconds(N) -&gt; MSecs</p><p>Types:</p><p>N = integer()</p><p>Value to convert to milliseconds.</p><p>Theese functions convert <em>N</em> number of hours, minutes or seconds into milliseconds.</p><p>Use this function when you want to <em>test_server:sleep/1</em> for a number of seconds, minutes or hours(!).</p><p><strong></strong> format(Format) -&gt; ok</p><p><strong></strong> format(Format, Args)</p><p><strong></strong> format(Pri, Format)</p><p><strong></strong> format(Pri, Format, Args)</p><p>Types:</p><p>Format = string()</p><p>Format as described for <em>io_:format</em>.</p><p>Args = list()</p><p>List of arguments to format.</p><p>Formats output just like <em>io:format</em> but sends the formatted string to a logfile. If the urgency value, <em>Pri</em>, is lower than some threshold value, it will also be written to the test person's console. Default urgency is 50, default threshold for display on the console is 1.</p><p>Typically, the test person don't want to see everything a test suite outputs, but is merely interested in if the test cases succeeded or not, which the test server tells him. If he would like to see more, he could manually change the threshold values by using the <em>test_server_ctrl:set_levels/3</em> function.</p><p><strong></strong> capture_start() -&gt; ok</p><p><strong></strong> capture_stop() -&gt; ok</p><p><strong></strong> capture_get() -&gt; list()</p><p>These functions makes it possible to capture all output to stdout from a process started by the test suite. The list of characters captured can be purged by using <em>capture_get</em>.</p><p><strong></strong> messages_get() -&gt; list()</p><p>This function will empty and return all the messages currently in the calling process' message queue.</p><p><strong></strong> timecall(M, F, A) -&gt; {Time, Value}</p><p>Types:</p><p>M = atom()</p><p>The name of the module where the function resides.</p><p>F = atom()</p><p>The name of the function to call in the module.</p><p>A = list()</p><p>The arguments to supply the called function.</p><p>Time = integer()</p><p>The number of seconds it took to call the function.</p><p>Value = term()</p><p>Value returned from the called function.</p><p>This function measures the time (in seconds) it takes to call a certain function. The function call is <em>not</em> caught within a catch.</p><p><strong></strong> do_times(N, M, F, A) -&gt; ok</p><p><strong></strong> do_times(N, Fun)</p><p>Types:</p><p>N = integer()</p><p>Number of times to call MFA.</p><p>M = atom()</p><p>Module name where the function resides.</p><p>F = atom()</p><p>Function name to call.</p><p>A = list()</p><p>Arguments to M:F.</p><p>Calls MFA or Fun N times. Useful for extensive testing of a sensitive function.</p><p><strong></strong> m_out_of_n(M, N, Fun) -&gt; ok | exit({m_out_of_n_failed, {R,left_to_do}}</p><p>Types:</p><p>N = integer()</p><p>Number of times to call the Fun.</p><p>M = integer()</p><p>Number of times to require a successful return.</p><p>Repeatedly evaluates the given function until it succeeds (doesn't crash) M times. If, after N times, M successful attempts have not been accomplished, the process crashes with reason {m_out_of_n_failed, {R,left_to_do}}, where R indicates how many cases that was still to be successfully completed.</p><p>For example:</p><p><em>m_out_of_n(1,4,fun() -&gt; tricky_test_case() end)</em></p><p>Tries to run tricky_test_case() up to 4 times, and is happy if it succeeds once.</p><p><em>m_out_of_n(7,8,fun() -&gt; clock_sanity_check() end)</em></p><p>Tries running clock_sanity_check() up to 8 times,and allows the function to fail once. This might be useful if clock_sanity_check/0 is known to fail if the clock crosses an hour boundary during the test (and the up to 8 test runs could never cross 2 boundaries)</p><p><strong></strong> call_crash(M, F, A) -&gt; Result</p><p><strong></strong> call_crash(Time, M, F, A) -&gt; Result</p><p><strong></strong> call_crash(Time, Crash, M, F, A) -&gt; Result</p><p>Types:</p><p>Result = ok | exit(call_crash_timeout) | exit({wrong_crash_reason, Reason})</p><p>Crash = term()</p><p>Crash return from the function.</p><p>Time = integer()</p><p>Timeout in milliseconds.</p><p>M = atom()</p><p>Module name where the function resides.</p><p>F = atom()</p><p>Function name to call.</p><p>A = list()</p><p>Arguments to M:F.</p><p>Spawns a new process that calls MFA. The call is considered successful if the call crashes with the gives reason (<em>Crash</em>) or any reason if not specified. The call must terminate within the given time (default <em>infinity</em>), or it is considered a failure.</p><p><strong></strong> temp_name(Stem) -&gt; Name</p><p>Types:</p><p>Stem = string()</p><p>Returns a unique filename starting with <em>Stem</em> with enough extra characters appended to make up a unique filename. The filename returned is guaranteed not to exist in the filesystem at the time of the call.</p><p><strong></strong> break(Comment) -&gt; ok</p><p>Types:</p><p>Comment = string()</p><p><em>Comment</em> is a string which will be written in the shell, e.g. explaining what to do.</p><p>This function will cancel all timetraps and pause the execution of the test case until the user executes the <em>continue/0</em> function. It gives the user the opportunity to interact with the erlang node running the tests, e.g. for debugging purposes or for manually executing a part of the test case.</p><p>When the <em>break/1</em> function is called, the shell will look something like this:</p>
<pre>
   --- SEMIAUTOMATIC TESTING ---
   The test case executes on process &lt;0.51.0&gt;


   "Here is a comment, it could e.g. instruct to pull out a card"


   -----------------------------

   Continue with --&gt; test_server:continue().
</pre>
<p>The user can now interact with the erlang node, and when ready call <em>test_server:continue().</em></p><p>Note that this function can not be used if the test is executed with <em>ts:run/0/1/2/3/4</em> in <em>batch</em> mode.</p><p><strong></strong> continue() -&gt; ok</p><p>This function must be called in order to continue after a test case has called <em>break/1</em>.</p><p><strong></strong> run_on_shielded_node(Fun, CArgs) -&gt; term()</p><p>Types:</p><p>Fun = function() (arity 0)</p><p>Function to execute on the shielded node.</p><p>CArg = string()</p><p>Extra command line arguments to use when starting the shielded node.</p><p><em>Fun</em> is executed in a process on a temporarily created hidden node with a proxy for communication with the test server node. The node is called a shielded node (should have been called a shield node). If <em>Fun</em> is successfully executed, the result is returned. A peer node (see <em>start_node/3</em>) started from the shielded node will be shielded from test server node, i.e. they will not be aware of each other. This is useful when you want to start nodes from earlier OTP releases than the OTP release of the test server node.</p><p>Nodes from an earlier OTP release can normally not be started if the test server hasn't been started in compatibility mode (see the <em>+R</em> flag in the <a href="../man1/erl.1.html"><strong>erl</strong>(1)</a></em> documentation) of an earlier release. If a shielded node is started in compatibility mode of an earlier OTP release than the OTP release of the test server node, the shielded node can start nodes of an earlier OTP release.</p><p><strong></strong> Note:</p><p>You <em>must</em> make sure that nodes started by the shielded node never communicate directly with the test server node.</p><p><strong></strong> Note:</p><p>Slave nodes always communicate with the test server node; therefore, <em>never</em> start <em>slave nodes</em> from the shielded node, <em>always</em> start <em>peer nodes</em>.</p><p><strong></strong> start_node(Name, Type, Options) -&gt; {ok, Node} | {error, Reason}</p><p>Types:</p><p>Name = atom() | string()</p><p>Name of the slavenode to start (as given to -sname or -name)</p><p>Type = slave | peer</p><p>The type of node to start.</p><p>Options = [{atom(), term()]</p><p>Tuplelist of options</p><p>This functions starts a node, possibly on a remote machine, and guarantees cross architecture transparency. Type is set to either <em>slave</em> or <em>peer</em>.</p><p><em>slave</em> means that the new node will have a master, i.e. the slave node will terminate if the master terminates, TTY output produced on the slave will be sent back to the master node and file I/O is done via the master. The master is normally the target node unless the target is itself a slave.</p><p><em>peer</em> means that the new node is an independent node with no master.</p><p><em>Options</em> is a tuplelist which can contain one or more of</p>
<dl class='dl-vertical'>
  <dt>
    <p><strong></strong></p>
  </dt>
  <dd>
    <p><em>{remote, true}</em>: Start the node on a remote host. If not specified, the node will be started on the local host. Test cases that require a remote host will fail with a reasonable comment if no remote hosts are available at the time they are run.</p>
  </dd>
  <dt>
    <p><strong></strong></p>
  </dt>
  <dd>
    <p><em>{args, Arguments}</em>: Arguments passed directly to the node. This is typically a string appended to the command line.</p>
  </dd>
  <dt>
    <p><strong></strong></p>
  </dt>
  <dd>
    <p><em>{wait, false}</em>: Don't wait until the node is up. By default, this function does not return until the node is up and running, but this option makes it return as soon as the node start command is given..</p><p> Only valid for peer nodes</p>
  </dd>
  <dt>
    <p><strong></strong></p>
  </dt>
  <dd>
    <p><em>{fail_on_error, false}</em>: Returns <em>{error, Reason}</em> rather than failing the test case.</p><p> Only valid for peer nodes. Note that slave nodes always act as if they had <em>fail_on_error=false</em></p>
  </dd>
  <dt>
    <p><strong></strong></p>
  </dt>
  <dd>
    <p><em>{erl, ReleaseList}</em>: Use an Erlang emulator determined by ReleaseList when starting nodes, instead of the same emulator as the test server is running. ReleaseList is a list of specifiers, where a specifier is either {release, Rel}, {prog, Prog}, or 'this'. Rel is either the name of a release, e.g., "r12b_patched" or 'latest'. 'this' means using the same emulator as the test server. Prog is the name of an emulator executable. If the list has more than one element, one of them is picked randomly. (Only works on Solaris and Linux, and the test server gives warnings when it notices that nodes are not of the same version as itself.)</p><p> When specifying this option to run a previous release, use <em>is_release_available/1</em> function to test if the given release is available and skip the test case if not.</p><p> In order to avoid compatibility problems (may not appear right away), use a shielded node (see <em>run_on_shielded_node/2</em>) when starting nodes from different OTP releases than the test server.</p>
  </dd>
  <dt>
    <p><strong></strong></p>
  </dt>
  <dd>
    <p><em>{cleanup, false}</em>: Tells the test server not to kill this node if it is still alive after the test case is completed. This is useful if the same node is to be used by a group of test cases.</p>
  </dd>
  <dt>
    <p><strong></strong></p>
  </dt>
  <dd>
    <p><em>{env, Env}</em>: <em>Env</em> should be a list of tuples <em>{Name, Val}</em>, where <em>Name</em> is the name of an environment variable, and <em>Val</em> is the value it is to have in the started node. Both <em>Name</em> and <em>Val</em> must be strings. The one exception is <em>Val</em> being the atom <em>false</em> (in analogy with <em>os:getenv/1</em>), which removes the environment variable. Only valid for peer nodes. Not available on VxWorks.</p>
  </dd>
  <dt>
    <p><strong></strong></p>
  </dt>
  <dd>
    <p><em>{start_cover, false}</em>: By default the test server will start cover on all nodes when the test is run with code coverage analysis. To make sure cover is not started on a new node, set this option to <em>false</em>. This can be necessary if the connection to the node at some point will be broken but the node is expected to stay alive. The reason is that a remote cover node can not continue to run without its main node. Another solution would be to explicitly stop cover on the node before breaking the connection, but in some situations (if old code resides in one or more processes) this is not possible.</p>
  </dd>

</dl>
<p><strong></strong> stop_node(NodeName) -&gt; bool()</p><p>Types:</p><p>NodeName = term()</p><p>Name of the node to stop</p><p>This functions stops a node previously started with <em>start_node/3</em>. Use this function to stop any node you start, or the test server will produce a warning message in the test logs, and kill the nodes automatically unless it was started with the <em>{cleanup, false}</em> option.</p><p><strong></strong> is_commercial() -&gt; bool()</p><p>This function test whether the emulator is commercially supported emulator. The tests for a commercially supported emulator could be more stringent (for instance, a commercial release should always contain documentation for all applications).</p><p><strong></strong> is_release_available(Release) -&gt; bool()</p><p>Types:</p><p>Release = string() | atom()</p><p>Release to test for</p><p>This function test whether the release given by <em>Release</em> (for instance, "r12b_patched") is available on the computer that the test_server controller is running on. Typically, you should skip the test case if not.</p><p>Caution: This function may not be called from the <em>suite</em> clause of a test case, as the test_server will deadlock.</p><p><strong></strong> is_native(Mod) -&gt; bool()</p><p>Types:</p><p>Mod = atom()</p><p>A module name</p><p>Checks whether the module is natively compiled or not</p><p><strong></strong> app_test(App) -&gt; ok | test_server:fail()</p><p><strong></strong> app_test(App,Mode)</p><p>Types:</p><p>App = term()</p><p>The name of the application to test</p><p>Mode = pedantic | tolerant</p><p>Default is pedantic</p><p>Checks an applications .app file for obvious errors. The following is checked:</p>
<dl class='dl-vertical'>
  <dt>
    <p>*</p>
  </dt>
  <dd>
    <p>required fields</p>
  </dd>

</dl>

<dl class='dl-vertical'>
  <dt>
    <p>*</p>
  </dt>
  <dd>
    <p>that all modules specified actually exists</p>
  </dd>

</dl>

<dl class='dl-vertical'>
  <dt>
    <p>*</p>
  </dt>
  <dd>
    <p>that all requires applications exists</p>
  </dd>

</dl>

<dl class='dl-vertical'>
  <dt>
    <p>*</p>
  </dt>
  <dd>
    <p>that no module included in the application has export_all</p>
  </dd>

</dl>

<dl class='dl-vertical'>
  <dt>
    <p>*</p>
  </dt>
  <dd>
    <p>that all modules in the ebin/ dir is included (If <em>Mode==tolerant</em> this only produces a warning, as all modules does not have to be included)</p>
  </dd>

</dl>
<p><strong></strong> appup_test(App) -&gt; ok | test_server:fail()</p><p>Types:</p><p>App = term()</p><p>The name of the application to test</p><p>Checks an applications .appup file for obvious errors. The following is checked:</p>
<dl class='dl-vertical'>
  <dt>
    <p>*</p>
  </dt>
  <dd>
    <p>syntax</p>
  </dd>

</dl>

<dl class='dl-vertical'>
  <dt>
    <p>*</p>
  </dt>
  <dd>
    <p>that .app file version and .appup file version match</p>
  </dd>

</dl>

<dl class='dl-vertical'>
  <dt>
    <p>*</p>
  </dt>
  <dd>
    <p>for non-library applications: validity of high-level upgrade instructions, specifying no instructions is explicitly allowed (in this case the application is not upgradeable)</p>
  </dd>

</dl>

<dl class='dl-vertical'>
  <dt>
    <p>*</p>
  </dt>
  <dd>
    <p>for library applications: that there is exactly one wildcard regexp clause restarting the application when upgrading or downgrading from any version</p>
  </dd>

</dl>
<p><strong></strong> comment(Comment) -&gt; ok</p><p>Types:</p><p>Comment = string()</p><p>The given String will occur in the comment field of the table on the HTML result page. If called several times, only the last comment is printed. comment/1 is also overwritten by the return value {comment,Comment} from a test case or by fail/1 (which prints Reason as a comment).</p>
        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">TEST SUITE EXPORTS</h2>
        <div class="sectioncontent">
<p>The following functions must be exported from a test suite module.</p>
        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">EXPORTS</h2>
        <div class="sectioncontent">
<p><strong></strong> all(suite) -&gt; TestSpec | {skip, Comment}</p><p>Types:</p><p>TestSpec = list()</p><p>Comment = string()</p><p>This comment will be printed on the HTML result page</p><p>This function must return the test specification for the test suite module. The syntax of a test specification is described in the Test Server User's Guide.</p><p><strong></strong> init_per_suite(Config0) -&gt; Config1 | {skip, Comment}</p><p>Types:</p><p>Config0 = Config1 = [tuple()]</p><p>Comment = string()</p><p>Describes why the suite is skipped</p><p>This function is called before all other test cases in the suite. <em>Config</em> is the configuration which can be modified here. Whatever is returned from this function is given as <em>Config</em> to the test cases.</p><p>If this function fails, all test cases in the suite will be skipped.</p><p><strong></strong> end_per_suite(Config) -&gt; void()</p><p>Types:</p><p>Config = [tuple()]</p><p>This function is called after the last test case in the suite, and can be used to clean up whatever the test cases have done. The return value is ignored.</p><p><strong></strong> init_per_testcase(Case, Config0) -&gt; Config1 | {skip, Comment}</p><p>Types:</p><p>Case = atom()</p><p>Config0 = Config1 = [tuple()]</p><p>Comment = string()</p><p>Describes why the test case is skipped</p><p>This function is called before each test case. The <em>Case</em> argument is the name of the test case, and <em>Config</em> is the configuration which can be modified here. Whatever is returned from this function is given as <em>Config</em> to the test case.</p><p><strong></strong> end_per_testcase(Case, Config) -&gt; void()</p><p>Types:</p><p>Case = atom()</p><p>Config = [tuple()]</p><p>This function is called after each test case, and can be used to clean up whatever the test case has done. The return value is ignored.</p><p><strong></strong> Case(doc) -&gt; [Decription]</p><p><strong></strong> Case(suite) -&gt; [] | TestSpec | {skip, Comment}</p><p><strong></strong> Case(Config) -&gt; {skip, Comment} | {comment, Comment} | Ok</p><p>Types:</p><p>Description = string()</p><p>Short description of the test case</p><p>TestSpec = list()</p><p>Comment = string()</p><p>This comment will be printed on the HTML result page</p><p>Ok = term()</p><p>Config = [tuple()]</p><p>Elements from the Config parameter can be read with the ?config macro, see section about test suite support macros</p><p>The <em>documentation clause</em> (argument <em>doc</em>) can be used for automatic generation of test documentation or test descriptions.</p><p>The <em>specification clause</em> (argument <em>spec</em>) shall return an empty list, the test specification for the test case or <em>{skip,Comment}</em>. The syntax of a test specification is described in the Test Server User's Guide.</p><p>The <em>execution clause</em> (argument <em>Config</em>) is only called if the specification clause returns an empty list. The execution clause is the real test case. Here you must call the functions you want to test, and do whatever you need to check the result. If something fails, make sure the process crashes or call <em>test_server:fail/0/1</em> (which also will cause the process to crash).</p><p>You can return <em>{skip,Comment}</em> if you decide not to run the test case after all, e.g. if it is not applicable on this platform.</p><p>You can return <em>{comment,Comment}</em> if you wish to print some information in the 'Comment' field on the HTML result page.</p><p>If the execution clause returns anything else, it is considered a success, unless it is <em>{'EXIT',Reason}</em> or <em>{'EXIT',Pid,Reason}</em> which can't be distinguished from a crash, and thus will be considered a failure.</p><p>A <em>conf test case</em> is a group of test cases with an init and a cleanup function. The init and cleanup functions are also test cases, but they have special rules:</p>
<dl class='dl-vertical'>
  <dt>
    <p>*</p>
  </dt>
  <dd>
    <p>They do not need a specification clause.</p>
  </dd>

</dl>

<dl class='dl-vertical'>
  <dt>
    <p>*</p>
  </dt>
  <dd>
    <p>They must always have the execution clause.</p>
  </dd>

</dl>

<dl class='dl-vertical'>
  <dt>
    <p>*</p>
  </dt>
  <dd>
    <p>They must return the <em>Config</em> parameter, a modified version of it or <em>{skip,Comment}</em> from the execution clause.</p>
  </dd>

</dl>

<dl class='dl-vertical'>
  <dt>
    <p>*</p>
  </dt>
  <dd>
    <p>The cleanup function may also return a tuple <em>{return_group_result,Status}</em>, which is used to return the status of the conf case to Test Server and/or to a conf case on a higher level. (<em>Status = ok | skipped | failed</em>).</p>
  </dd>

</dl>

<dl class='dl-vertical'>
  <dt>
    <p>*</p>
  </dt>
  <dd>
    <p><em>init_per_testcase</em> and <em>end_per_testcase</em> are not called before and after these functions.</p>
  </dd>

</dl>

        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">TEST SUITE LINE NUMBERS</h2>
        <div class="sectioncontent">
<p>If a test case fails, the test server can report the exact line number at which it failed. There are two ways of doing this, either by using the <em>line</em> macro or by using the <em>test_server_line</em> parse transform.</p><p>The <em>line</em> macro is described under TEST SUITE SUPPORT MACROS below. The <em>line</em> macro will only report the last line executed when a test case failed.</p><p>The <em>test_server_line</em> parse transform is activated by including the headerfile <em>test_server_line.hrl</em> in the test suite. When doing this, it is important that the <em>test_server_line</em> module is in the code path of the erlang node compiling the test suite. The parse transform will report a history of a maximum of 10 lines when a test case fails. Consecutive lines in the same function are not shown.</p><p>The attribute <em>-no_lines(FuncList).</em> can be used in the test suite to exclude specific functions from the parse transform. This is necessary e.g. for functions that are executed on old (i.e. &lt;R10B) OTP releases. <em>FuncList = [{Func,Arity}]</em>.</p><p>If both the <em>line</em> macro and the parse transform is used in the same module, the parse transform will overrule the macro.</p>
        </div>
      </section>

      <section>
        <h2 class="font-effect-shadow-multiple">TEST SUITE SUPPORT MACROS</h2>
        <div class="sectioncontent">
<p>There are some macros defined in the <em>test_server.hrl</em> that are quite useful for test suite programmers:</p><p>The <em>line</em> macro, is quite essential when writing test cases. It tells the test server exactly what line of code that is being executed, so that it can report this line back if the test case fails. Use this macro at the beginning of every test case line of code.</p><p>The <em>config</em> macro, is used to retrieve information from the <em>Config</em> variable sent to all test cases. It is used with two arguments, where the first is the name of the configuration variable you wish to retrieve, and the second is the <em>Config</em> variable supplied to the test case from the test server.</p><p>Possible configuration variables include:</p>
<dl class='dl-vertical'>
  <dt>
    <p>*</p>
  </dt>
  <dd>
    <p><em>data_dir</em> - Data file directory.</p>
  </dd>

</dl>

<dl class='dl-vertical'>
  <dt>
    <p>*</p>
  </dt>
  <dd>
    <p><em>priv_dir</em> - Scratch file directory.</p>
  </dd>

</dl>

<dl class='dl-vertical'>
  <dt>
    <p>*</p>
  </dt>
  <dd>
    <p><em>nodes</em> - Nodes specified in the spec file</p>
  </dd>

</dl>

<dl class='dl-vertical'>
  <dt>
    <p>*</p>
  </dt>
  <dd>
    <p><em>nodenames</em> - Generated nodenames.</p>
  </dd>

</dl>

<dl class='dl-vertical'>
  <dt>
    <p>*</p>
  </dt>
  <dd>
    <p>Whatever added by conf test cases or <em>init_per_testcase/2</em></p>
  </dd>

</dl>
<p>Examples of the <em>line</em> and <em>config</em> macros can be seen in the Examples chapter in the user's guide.</p><p>If the <em>line_trace</em> macro is defined, you will get a timestamp (<em>erlang:now()</em>) in your minor log for each <em>line</em> macro in your suite. This way you can at any time see which line is currently being executed, and when the line was called.</p><p>The <em>line_trace</em> macro can also be used together with the <em>test_server_line</em> parse transform described above. A timestamp will then be written for each line in the suite, except for functions stated in the <em>-no_lines</em> attribute.</p><p>The <em>line_trace</em> macro can e.g. be defined as a compile option, like this:</p><p><em>erlc -W -Dline_trace my_SUITE.erl</em></p>
        </div>
      </section>
<nav>
  <ul class="pager">
   <li class="previous"><a href="tags.3erl.html"><span aria-hidden="true">&larr;</span> tags.3erl: Generate emacs tags file from erlang source files</a></li>
   <li class="next"><a href="test_server_ctrl.3erl.html">test_server_ctrl.3erl: This module provides a low level interface to the test server. <span aria-hidden="true">&rarr;</span></a></li>
  </ul>
</nav>

  </div>
  <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
  <!-- Include all compiled plugins (below), or include individual files as needed -->
  <script src="/js/bootstrap.min.js"></script>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-60781387-1', 'auto');
    ga('send', 'pageview');

  </script>
</body>
</html>
